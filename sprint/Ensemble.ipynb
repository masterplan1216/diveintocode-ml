{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sprintの目的\n",
    "- アンサンブル学習について理解する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#np.set_printoptions(threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3種類のアンサンブル学習をスクラッチ実装していきます。そして、それぞれの効果を小さめのデータセットで確認します。\n",
    "\n",
    "\n",
    "- ブレンディング\n",
    "- バギング\n",
    "- スタッキング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以前も利用した回帰のデータセットを用意します。\n",
    "\n",
    "\n",
    "[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使います。\n",
    "\n",
    "\n",
    "train.csvを学習用（train）8割、検証用（val）2割に分割してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data = pd.read_csv('train.csv')\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1710, 2003],\n",
       "       [1262, 1976],\n",
       "       [1786, 2001],\n",
       "       ...,\n",
       "       [2340, 1941],\n",
       "       [1078, 1950],\n",
       "       [1256, 1965]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([208500, 181500, 223500, ..., 266500, 142125, 147500])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = original_data.loc[:, [\"GrLivArea\", \"YearBuilt\"]].values\n",
    "display(X)\n",
    "print(X.shape)\n",
    "\n",
    "y = original_data[\"SalePrice\"].values\n",
    "display(y)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単一のモデルはスクラッチ実装ではなく、scikit-learnなどのライブラリの使用を推奨します。\n",
    "\n",
    "[sklearn.linear_model.LinearRegression — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "\n",
    "[sklearn.svm.SVR — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "\n",
    "\n",
    "[sklearn.tree.DecisionTreeRegressor — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】ブレンディングのスクラッチ実装\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。\n",
    "\n",
    "ブレンディングとは、N個の多様なモデルを独立して学習させ、推定結果を重み付けした上で足し合わせる方法です。最も単純には平均をとります。多様なモデルとは、以下のような条件を変化させることで作り出すものです。\n",
    "\n",
    "\n",
    "- 手法（例：線形回帰、SVM、決定木、ニューラルネットワークなど）\n",
    "- ハイパーパラメータ（例：SVMのカーネルの種類、重みの初期値など）\n",
    "- 入力データの前処理の仕方（例：標準化、対数変換、PCAなど）\n",
    "\n",
    "重要なのはそれぞれのモデルが大きく異なることです。\n",
    "\n",
    "\n",
    "回帰問題でのブレンディングは非常に単純であるため、scikit-learnには用意されていません。\n",
    "\n",
    "\n",
    "《補足》\n",
    "\n",
    "\n",
    "分類問題の場合は、多数決を行います。回帰問題に比べると複雑なため、scikit-learnにはVotingClassifierが用意されています。\n",
    "\n",
    "\n",
    "[sklearn.ensemble.VotingClassifier — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 単一モデルでのMSE(平均二乗誤差)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測値：\n",
      " [264908.90812295 155745.99630863 127984.53226316 ... 223602.56439896\n",
      "  68715.66448996  87870.91973285]\n",
      "MSE： 2,942,066,921.67\n"
     ]
    }
   ],
   "source": [
    "#線形回帰\n",
    "np.set_printoptions(threshold=0)\n",
    "\n",
    "lr =  LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"予測値：\\n\", lr.predict(X_val))\n",
    "print(\"MSE：\", f\"{mean_squared_error(y_val, lr.predict(X_val)):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測値：\n",
      " [162999.39962414 162999.31753914 162999.39962041 ... 162999.39962414\n",
      " 162999.39962414 162999.39962412]\n",
      "MSE： 7,243,319,908.93\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)\n",
    "print(\"予測値：\\n\", svr.predict(X_val))\n",
    "print(\"MSE：\", f\"{mean_squared_error(y_val, svr.predict(X_val)):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測値：\n",
      " [151400. 140000. 125000. ... 169000. 118000. 116900.]\n",
      "MSE： 3,374,967,201.20\n"
     ]
    }
   ],
   "source": [
    "#決定木\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"予測値：\\n\", tree.predict(X_val))\n",
    "print(\"MSE：\", f\"{mean_squared_error(y_val, tree.predict(X_val)):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ブレンディングクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blending_model():\n",
    "    \"\"\"\n",
    "    ブレンディングのスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, estimators):\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        N個の多様なモデルを独立して学習させ、辞書に保存する\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray(n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : ndarray(n_samples, )\n",
    "            正解データ\n",
    "        estimators : [model1(), model2(), ...] \n",
    "            学習させるモデル\n",
    "        \n",
    "        \"\"\"\n",
    "        self.estimator_dic = {}\n",
    "        \n",
    "        for i, estimator in enumerate(self.estimators):\n",
    "            model = estimator #モデルのインスタンス化\n",
    "            self.estimator_dic[i] = model.fit(X, y) #fitさせたモデルを辞書に保存\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        辞書に保存された各学習モデルの予測値を算出後、その平均値を計算\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray(n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        \n",
    "        Return\n",
    "        ----------\n",
    "        y_pred : ndarray(n_samples, )\n",
    "            検証データの予測値\n",
    "        \"\"\"\n",
    "        predict_list = []\n",
    "        \n",
    "        for _, model in self.estimator_dic.items():\n",
    "            predict_list.append(model.predict(X))\n",
    "        \n",
    "        #print(\"predict_list:\\n\", predict_list)\n",
    "        \n",
    "        y_pred = np.mean(predict_list, axis=0)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ブレンドモデルでのMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測値：\n",
      " [193102.76924903 152981.77128259 138661.31062786 ... 185200.65467437\n",
      " 116571.68803803 122590.10645233]\n",
      "---------\n",
      "線形回帰MSE： 2,942,066,921.67\n",
      "MSE： 2,900,940,898.78\n"
     ]
    }
   ],
   "source": [
    "#検証1\n",
    "#「線形回帰、SVM、決定木」のブレンド\n",
    "\n",
    "bm = Blending_model(estimators=[LinearRegression(), SVR(), DecisionTreeRegressor()])\n",
    "bm.fit(X_train, y_train)\n",
    "print(\"予測値：\\n\", bm.predict(X_val))\n",
    "print(\"---------\")\n",
    "print(\"線形回帰MSE：\", f\"{mean_squared_error(y_val, lr.predict(X_val)):,.2f}\")\n",
    "print(\"MSE：\", f\"{mean_squared_error(y_val, bm.predict(X_val)):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単一モデルで一番良かった線形回帰より、僅かに精度の良いモデルとなった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測値：\n",
      " [213954.15387355 159372.65692389 145491.96594178 ... 193300.98201155\n",
      " 115857.53205705 125435.15967849]\n",
      "---------\n",
      "線形回帰MSE： 2,942,066,921.67\n",
      "MSE： 3,776,403,745.45\n"
     ]
    }
   ],
   "source": [
    "#検証2\n",
    "#「線形回帰、SVM」のブレンド\n",
    "\n",
    "bm = Blending_model(estimators=[LinearRegression(), SVR()])\n",
    "bm.fit(X_train, y_train)\n",
    "print(\"予測値：\\n\", bm.predict(X_val))\n",
    "print(\"---------\")\n",
    "print(\"線形回帰MSE：\", f\"{mean_squared_error(y_val, lr.predict(X_val)):,.2f}\")\n",
    "print(\"MSE：\", f\"{mean_squared_error(y_val, bm.predict(X_val)):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精度の悪いモデルと良いモデルをブレンドすると精度が悪くなることは想像しやすい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測値：\n",
      " [208154.45406148 147972.99815432 126492.26613158 ... 196301.28219948\n",
      "  93357.83224498  99185.45986643]\n",
      "---------\n",
      "線形回帰MSE： 2,942,066,921.67\n",
      "MSE： 2,575,081,763.46\n"
     ]
    }
   ],
   "source": [
    "#検証3\n",
    "#「線形回帰、決定木」のブレンド\n",
    "\n",
    "bm = Blending_model(estimators=[LinearRegression(), DecisionTreeRegressor()])\n",
    "bm.fit(X_train, y_train)\n",
    "print(\"予測値：\\n\", bm.predict(X_val))\n",
    "print(\"---------\")\n",
    "print(\"線形回帰MSE：\", f\"{mean_squared_error(y_val, lr.predict(X_val)):,.2f}\")\n",
    "print(\"MSE：\", f\"{mean_squared_error(y_val, bm.predict(X_val)):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一番良いブレンドモデルができている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測値：\n",
      " [12.17706851 11.76949193 11.7382464  ... 12.05406506 11.55745489\n",
      " 11.59022568]\n",
      "---------\n",
      "線形回帰MSE： 2,910,319,370.76\n",
      "MSE： 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#検証4\n",
    "#前処理をした「線形回帰、決定木」のブレンド\n",
    "\n",
    "#標準化、対数変換\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "y_train_scaled = np.log(y_train)\n",
    "y_val_scaled = np.log(y_val)\n",
    "\n",
    "bm = Blending_model(estimators=[LinearRegression(), DecisionTreeRegressor()])\n",
    "bm.fit(X_train_scaled, y_train_scaled)\n",
    "print(\"予測値：\\n\", bm.predict(X_val_scaled))\n",
    "print(\"---------\")\n",
    "print(\"線形回帰MSE：\", f\"{mean_squared_error(y_val, lr.predict(X_val)):,.2f}\")\n",
    "print(\"MSE：\", f\"{mean_squared_error(y_val_scaled, bm.predict(X_val_scaled)):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検証1、3、4で精度が上がっていることを確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参考サイト\n",
    "\n",
    "[Blending.1](https://www.quora.com/What-is-blending-in-machine-learning)  \n",
    ">機械学習におけるブレンドとは何ですか？\n",
    "\n",
    ">短い答えは次のとおりです。多くの個別のモデルを使用して初期予測を計算し、さらにいくつかの方法で予測を混合して、さらに優れた最終予測を達成する方法。\n",
    ">\n",
    ">より正確に言うと、たとえばチームで問題に取り組んだ場合。コンテストの場合、誰もが独自のモデルに取り組んでいます（人々はアイデアについて話し合いますが、誰もが自分のモデルを所有およびコーディングしています）。その後、通常は単純なニューラルネットワークを使用してすべての結果を混合します。入力は問題の説明とさまざまなモデルの結果であり、出力は最終予測です。\n",
    ">\n",
    ">時々、単に各モデルの予測の加重平均を使用するだけで十分であり、ニューラルネットワークの必要性を排除します。\n",
    ">\n",
    ">時には、さらに複雑な方法が使用されます。詳細については、Rich Caruanaによる「モデルのライブラリからのアンサンブルの選択」を読むことをお勧めします。\n",
    "\n",
    "[Blending.2](https://mlwave.com/kaggle-ensembling-guide/)\n",
    ">アンサンブルを行う最も基本的で便利な方法は、Kaggle送信CSVファイルをアンサンブルすることです。これらのメソッドのテストセットの予測のみが必要です。モデルを再トレーニングする必要はありません。これにより、既存のモデル予測をアンサンブルする迅速な方法となり、チーム化するときに理想的です。\n",
    "\n",
    "[Blending.3](https://www.datarobot.com/wiki/training-validation-holdout/)  \n",
    "\n",
    "\n",
    "[Blending.4](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44588)   \n",
    ">「面白い大会でした。私たちのチームは銀メダルを獲得しました。Kaggleの初心者にとって、それは私たちにとってかなり良い結果でした。\n",
    ">\n",
    ">私は議論を見ていて、多くの有用な提案を得ました。しかし、ブレンドとスタックが何であるかはよくわかりません。使用したアンサンブルは、すべてのサンプルで同じ係数を使用した予測の線形結合であり、トレーニングセットを検証セットに分割して係数を選択しました。これはブレンドですか？次に、スタッキングとの違いは何ですか？アンサンブル法を「linear-combination-method」と呼びました。正式名称はありますか？私はOOFと呼ばれるメソッドを見ましたが、それが何であるかわかりません（そしてGoogleで結果を見つけることができません）。また、これに使用するパッケージがあるかと思っていました。私は自分でブレンディングをコーディングしました。\n",
    ">\n",
    ">ありがとう！」  \n",
    "\n",
    "\n",
    ">「おめでとうございます！\n",
    ">\n",
    ">私がそれを正しく理解していれば、あなたがしたことは、人々が「ブレンド」によってしばしば意味することであったようにあなたは正しいようです。人々はこれらの用語の多くをかなり大まかに使用しているため、しばしばかなり不明確です。ここに私が標準的な定義として考えるものがあります：\n",
    ">\n",
    ">**ブレンド**：  \n",
    ">トレーニングデータの一部を保持します（たとえば、80/20の分割）。80パーツでベースモデルをトレーニングし、20パーツとテストセットで予測します。機能として20セットの予測を使用してメタラーナーをトレーニングし、最終的な提出予測のテストセットでメタラーナーを実行します。\n",
    ">\n",
    ">**スタッキング**：  \n",
    ">トレーニングデータをフォールドに分割します（たとえば5）。各トレーニングフォールドでベースモデルをトレーニングし、各検証フォールドで予測し、これらの予測を収集します（これがOOF部分の出所です。このようにして、トレーニングデータセット全体に対して行われた予測を収集しますが、「フォールド外」の予測であるため、モデルは、予測したデータに基づいてトレーニングされていませんでした）。次に、これらのOOF予測についてメタラーナーをトレーニングし、最終予測のためにテストセットでメタラーナーを実行します。\n",
    ">\n",
    ">この包括的なガイドは、役に立つリソースかもしれません。\n",
    ">\n",
    ">再パッケージ化-多くの人々は（単純な）スタッキングを手作業でコーディングしますが、StackNetに興味があるかもしれません。」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】バギングのスクラッチ実装\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "\n",
    "バギングは入力データの選び方を多様化する方法です。学習データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ ブートストラップサンプル ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "\n",
    "[sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "\n",
    "scikit-learnのtrain_test_splitを、shuffleパラメータをTrueにして使うことで、ランダムにデータを分割することができます。これによりブートストラップサンプルが手に入ります。\n",
    "\n",
    "\n",
    "推定結果の平均をとる部分はブースティングと同様の実装になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### バギングクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagging_model():\n",
    "    \"\"\"\n",
    "    ブレンディングのスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, boot_num):\n",
    "        self.estimator = estimator\n",
    "        self.boot_num = boot_num\n",
    "        \n",
    "    def _bootstrap(self, X, y):\n",
    "        \"\"\"\n",
    "        N種類のサブセット（ ブートストラップサンプル ）を作り出す\n",
    "        学習データから重複を許した上でランダムに抜き出すことで、N個の学習データを作成する\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray(n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : ndarray(n_samples, )\n",
    "            正解データ\n",
    "        \"\"\"\n",
    "        self.boot_Xdic = {}\n",
    "        self.boot_ydic = {}\n",
    "        \n",
    "        #訓練データから重複有りでランダム抽出して、指定個数分を辞書に保存\n",
    "        for i in range(self.boot_num):\n",
    "            idx = np.random.choice(X.shape[0], X.shape[0])\n",
    "            self.boot_Xdic[i] = X[idx]\n",
    "            self.boot_ydic[i] = y[idx]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        N個のサブセットを独立して学習させて、各学習モデルを辞書に保存\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray(n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : ndarray(n_samples, )\n",
    "            正解データ\n",
    "        estimators : [model1(), model2(), ...] \n",
    "            学習させるモデル\n",
    "        \n",
    "        \"\"\"\n",
    "        self._bootstrap(X, y) #ブートストラップサンプル作成\n",
    "        self.estimator_dic = {}\n",
    "        \n",
    "        for i in range(self.boot_num):\n",
    "            model = self.estimator #モデルのインスタンス化\n",
    "            self.estimator_dic[i] = model.fit(self.boot_Xdic[i], self.boot_ydic[i]) #fitさせたモデルを辞書に保存\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        辞書に保存された各学習モデルの予測値を算出後、その平均値を計算\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray(n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        \n",
    "        Return\n",
    "        ----------\n",
    "        y_pred : ndarray(n_samples, )\n",
    "            検証データの予測値\n",
    "        \"\"\"\n",
    "        predict_list = []\n",
    "        \n",
    "        for _, model in self.estimator_dic.items():\n",
    "            predict_list.append(model.predict(X))\n",
    "        \n",
    "        #print(\"predict_list:\\n\", predict_list)\n",
    "        \n",
    "        y_pred = np.mean(predict_list, axis=0)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測値：\n",
      " [278000. 140200. 135000. ... 133900. 118000.  68400.]\n",
      "---------\n",
      "決定木単一 MSE： 3,374,967,201.20\n",
      "決定木バギング MSE： 3,041,810,038.78\n"
     ]
    }
   ],
   "source": [
    "#単一な決定木モデルとバギング決定木モデルの精度比較\n",
    "\n",
    "bag_tree = Bagging_model(estimator=DecisionTreeRegressor(), boot_num=3)\n",
    "bag_tree.fit(X_train, y_train)\n",
    "print(\"予測値：\\n\", bag_tree.predict(X_val))\n",
    "print(\"---------\")\n",
    "print(\"決定木単一 MSE：\", f\"{mean_squared_error(y_val, tree.predict(X_val)):,.2f}\")\n",
    "print(\"決定木バギング MSE：\", f\"{mean_squared_error(y_val, bag_tree.predict(X_val)):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バギングをすることで精度が上がった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       " 1: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       " 2: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       " 3: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       " 4: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_tree.estimator_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】スタッキングのスクラッチ実装\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは$K_0 = 3, M_0 = 2$程度にします。\n",
    "\n",
    "\n",
    "《学習時》\n",
    "\n",
    "（ステージ 0）\n",
    "- 学習データを$K_0$個に分割する。\n",
    "- 分割した内の$(K_0 -1)$個をまとめて学習用データ、残り1個を推定用データとする組み合わせが$K_0$個作れる。\n",
    "- あるモデルのインスタンスを$K_0$個用意し、異なる学習用データを使い学習する。\n",
    "- それぞれの学習済みモデルに対して、使っていない残り1個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）\n",
    "- さらに、異なるモデルのインスタンスも$K_0$個用意し、同様のことを行う。モデルが$M_0$個あれば、$M_0$個のブレンドデータが得られる。\n",
    "\n",
    "（ステージn）\n",
    "- ステージ$n -1$のブレンドデータを$M_{n-1}$次元の特徴量を持つ学習用データと考え、$K_n$個に分割する。以下同様である。\n",
    "\n",
    "（ステージN）＊最後のステージ\n",
    "- ステージ$n -1$個のブレンドデータを$M_{n-1}$次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。\n",
    "\n",
    "《推定時》\n",
    "\n",
    "（ステージ0）\n",
    "- テストデータを$K_n$×$M_n$個の学習済みモデルに入力し、$K_n$×$M_n$個の推定値を得る。これを$K_0$の軸で平均値を求め$M_0$次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージn）\n",
    "- ステージ$n -1$で得たブレンドテストを$K_n$×$M_n$個の学習済みモデルに入力し、$K_n$×$M_n$個の推定値を得る。これを$K_n$の軸で平均値を求め$M_0$次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージN）＊最後のステージ\n",
    "- ステージ$N -1$で得たブレンドテストを学習済みモデルに入力し、推定値を得る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### スタッキングクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class Stacking_model():\n",
    "    \"\"\"\n",
    "    スタッキングのスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, estimators, split_size=3):\n",
    "        self.estimators = estimators\n",
    "        self.split_size = split_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.fit_model_lst = [] #学習済モデル\n",
    "        pred_list = [] #ブレンドデータの推定値\n",
    "        y_val_list = []\n",
    "        \n",
    "        #分割データで各モデルの学習と推定\n",
    "        kf = KFold(n_splits=self.split_size, random_state=0, shuffle=True)\n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            X_train_kf, X_val = X[train_idx], X[val_idx]\n",
    "            y_train_kf, y_val = y[train_idx], y[val_idx]\n",
    "            for model in deepcopy(self.estimators):\n",
    "                model_fitted = model.fit(X_train_kf, y_train_kf) #学習\n",
    "                self.fit_model_lst.append(model_fitted)\n",
    "\n",
    "                pred = model_fitted.predict(X_val) #ブレンドデータの推定値                 \n",
    "                pred_list.append(pred)\n",
    "            y_val_list.extend(y_val) \n",
    "        \n",
    "        blend_data = np.vstack((np.concatenate((pred_list[i::len(self.estimators)])) for i in range(len(self.estimators)))).T\n",
    "        \n",
    "        self.final_fitted = LinearRegression().fit(blend_data, np.array(y_val_list).reshape(-1, 1)) #最終モデル\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_list = []\n",
    "        for model_fitted in self.fit_model_lst:\n",
    "            pred = model_fitted.predict(X)\n",
    "            pred_list.append(pred)\n",
    "        \n",
    "        blend_test = np.vstack(np.mean(np.array(pred_list[i::len(self.estimators)]), axis=0) for i in range(len(self.estimators))).T\n",
    "        last_pred = self.final_fitted.predict(blend_test)\n",
    "        return last_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測値：\n",
      " [[248541.26697303]\n",
      " [153349.80868894]\n",
      " [127338.92042433]\n",
      " ...\n",
      " [217732.65514516]\n",
      " [ 70970.34548431]\n",
      " [ 92513.48144455]]\n",
      "---------\n",
      "線形回帰MSE： 2,942,066,921.67\n",
      "スタッキングMSE： 2,626,882,871.86\n"
     ]
    }
   ],
   "source": [
    "#最終モデルは線形回帰\n",
    "\n",
    "st = Stacking_model(estimators=[LinearRegression(), DecisionTreeRegressor()])\n",
    "st.fit(X_train, y_train)\n",
    "\n",
    "print(\"予測値：\\n\", st.predict(X_val))\n",
    "print(\"---------\")\n",
    "print(\"線形回帰MSE：\", f\"{mean_squared_error(y_val, lr.predict(X_val)):,.2f}\")\n",
    "print(\"スタッキングMSE：\", f\"{mean_squared_error(y_val, st.predict(X_val)):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スタッキングにより単一モデルより精度があがることを確認した。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
