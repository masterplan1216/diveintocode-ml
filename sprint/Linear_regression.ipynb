{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sprintの目的\n",
    "- スクラッチを通して線形回帰を理解する\n",
    "- オブジェクト指向を意識した実装に慣れる\n",
    "- 数式をコードに落とし込めるようにする\n",
    "\n",
    "スクラッチで線形回帰を実装した後、学習と検証を行なっていきます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形回帰のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "以下に雛形を用意してあります。このScratchLinearRegressionクラスにコードを書き加えていってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(自分用メモ) 重回帰と勾配降下法](https://gneioagine.hatenablog.com/entry/20170304/1488588334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最新版\n",
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ(重み)\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr=0.01, no_bias=True, verbose=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        if self.no_bias == False:\n",
    "            self.coef_ = np.random.rand(X.shape[1] + 1) #重み初期値：0以上1未満の乱数\n",
    "        else:\n",
    "            self.coef_ = np.random.rand(X.shape[1]) #重み初期値：0以上1未満の乱数\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            #仮定関数の値（hypo_y）\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            #誤差の算出 shape(m, 1)\n",
    "            error = y_pred - y.reshape(-1, 1)\n",
    "            #重みを更新\n",
    "            self.coef_ = self._gradient_descent(X, error)\n",
    "            print(self.coef_)\n",
    "            #目的（損失）関数の値\n",
    "            self.loss[i] = self.objective_func(X, error)\n",
    "            \n",
    "            #検証データがある場合のloss値保存\n",
    "            if X_val is not None:\n",
    "                self.val_loss = self.objective_func(X_val, y_val)\n",
    "        \n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print(\"train_loss:{}\".format(self.loss))\n",
    "            if X_val is not None:\n",
    "                print(\"val_loss:{}\".format(self.val_loss))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        hypo_y = self._linear_hypothesis(X)\n",
    "        \n",
    "        return hypo_y\n",
    "    \n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hypo_theta：次の形のndarray, shape (n_samples, 1) Q.この形にする意味は？？\n",
    "         線形の仮定関数による推定結果\n",
    "         予測値が返っている\n",
    "        \"\"\"\n",
    "        if self.no_bias == False:\n",
    "            #左列に1の列追加(切片用)\n",
    "            one = np.ones(X.shape[0]).reshape(-1, 1)\n",
    "            X = np.concatenate((one, X), axis=1)\n",
    "\n",
    "        #θ：重み（初期値：0以上1未満の乱数）\n",
    "        #self.coef_ = np.random.rand(X.shape[1])\n",
    "\n",
    "        #線形回帰の仮定関数\n",
    "        hypo_y = np.dot(X, self.coef_.reshape(-1, 1)) #(-1, 1)\n",
    "    \n",
    "        return hypo_y\n",
    "    \n",
    "    def _gradient_descent(self, X, error):\n",
    "        \"\"\"\n",
    "        重みの更新\n",
    "        Parameters\n",
    "        --------\n",
    "        X：次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        error：仮定関数の値　- 実際の値(n_samples, 1)\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        new_coef_：次の形のndarray, shape (1, n_features)\n",
    "        \"\"\"\n",
    "        m = X.shape[0] #サンプル数\n",
    "        \n",
    "        if self.no_bias == False:\n",
    "            one = np.ones(X.shape[0]).reshape(-1, 1)\n",
    "            X = np.concatenate((one, X), axis=1)\n",
    "            \n",
    "        new_coef_ = self.coef_ - (self.lr*(1/m)*np.dot(error.reshape(-1, ), X)) #(coef数, )\n",
    "\n",
    "        return new_coef_\n",
    "    \n",
    "    def objective_func(self, X, error):\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          サンプル\n",
    "        error：仮定関数の値　- 実際の値(n_samples, 1)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        loss : numpy.float\n",
    "          平均二乗誤差/2\n",
    "        \"\"\"\n",
    "        m = X.shape[0] #サンプルサイズ\n",
    "\n",
    "        loss = (1/(2*m))*np.sum(np.square(error)) #float\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#編集前\n",
    "class ScratchLinearRegression2222222222():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ(重み)\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr=0.01, no_bias=True, verbose=False):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        self.coef_ = np.random.rand(X.shape[1] + 1) #重み初期値：0以上1未満の乱数\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            #重みを更新\n",
    "            self.coef_ = self._gradient_descent(X, y)\n",
    "            print(self.coef_)\n",
    "            #目的（損失）関数の値\n",
    "            self.loss[i] = self.objective_func(X, y)\n",
    "            \n",
    "            #検証データがある場合のloss値保存\n",
    "            if X_val is not None:\n",
    "                self.val_loss = self.objective_func(X_val, y_val)\n",
    "        \n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程を出力\n",
    "            print(\"train_loss:{}\".format(self.loss))\n",
    "            if X_val is not None:\n",
    "                print(\"val_loss:{}\".format(self.val_loss))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        hypo_y, X = self._linear_hypothesis(X)\n",
    "        \n",
    "        return hypo_y\n",
    "    \n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hypo_theta：次の形のndarray, shape (n_samples, 1) Q.この形にする意味は？？\n",
    "         線形の仮定関数による推定結果\n",
    "         予測値が返っている\n",
    "        \"\"\"\n",
    "        #左列に1の列追加(切片用)\n",
    "        one = np.ones(X.shape[0]).reshape(-1, 1)\n",
    "        X_ = np.concatenate((one, X), axis=1)\n",
    "\n",
    "        #θ：重み（初期値：0以上1未満の乱数）\n",
    "        #self.coef_ = np.random.rand(X.shape[1])\n",
    "\n",
    "        #線形回帰の仮定関数\n",
    "        hypo_y = np.dot(X_, self.coef_.reshape(-1, 1)) #(-1, 1)\n",
    "    \n",
    "        return hypo_y, X_\n",
    "    \n",
    "    def _gradient_descent(self, X, y):\n",
    "        \"\"\"\n",
    "        重みの更新\n",
    "        Parameters\n",
    "        --------\n",
    "        X：次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データ\n",
    "        y：目的変数のndarray(n_features, )\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        new_coef_：次の形のndarray, shape (1, n_features)\n",
    "        \"\"\"\n",
    "        hypo_y, X_ = self._linear_hypothesis(X)\n",
    "        m = len(y) #サンプル数\n",
    "        \n",
    "        error = hypo_y - y.reshape(-1, 1) #誤差の算出 shape(m, 1)\n",
    "\n",
    "        new_coef_ = self.coef_ - (self.lr*(1/m)*np.dot(error.reshape(-1, ), X_)) #(coef数, )\n",
    "\n",
    "        return new_coef_\n",
    "    \n",
    "    def objective_func(self, X, y):\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          サンプル\n",
    "        y : 次の形のndarray, shape (n_samples,)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        loss : numpy.float\n",
    "          平均二乗誤差/2\n",
    "        \"\"\"\n",
    "        m = len(y) #サンプルサイズ\n",
    "        y_pred, X_ = self._linear_hypothesis(X)\n",
    "\n",
    "        loss = (1/(2*m))*np.sum(np.square(y_pred - y.reshape(-1, 1))) #float\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】仮定関数\n",
    "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。\n",
    "\n",
    "$$\n",
    "h_θ(x) = θ_0x_0 + θ_1x_1 + ... + θ_jx_j + ... + θ_nx_n (x_0 = 1)\n",
    "$$\n",
    "\n",
    "x : 特徴量ベクトル\n",
    "\n",
    "θ: パラメータベクトル\n",
    "\n",
    "n: 特徴量の数\n",
    "\n",
    "$x_j$ : j番目の特徴量\n",
    "\n",
    "$θ_j$: j番目のパラメータ（重み）\n",
    "\n",
    "特徴量の数 n は任意の値に対応できる実装にしてください。\n",
    "\n",
    "\n",
    "なお、ベクトル形式で表すと以下のようになります。\n",
    "$$\n",
    "h_θ(x) = θ^Tx\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _linear_hypothesis(self, X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hypo_theta：次の形のndarray, shape (n_samples, 1) Q.この形にする意味は？？\n",
    "     線形の仮定関数による推定結果\n",
    "     予測値が返っている\n",
    "    \"\"\"\n",
    "    #左列に1の列追加(切片用)\n",
    "    one = np.ones(X.shape[0]).reshape(-1, 1)\n",
    "    X = np.concatenate((one, X), axis=1)\n",
    "    \n",
    "    #θ：重み（初期値：0以上1未満の乱数）\n",
    "    self.coef_ = np.random.rand(X.shape[1])\n",
    "    \n",
    "    #線形回帰の仮定関数\n",
    "    hypo_y = np.dot(X, self.coef_.reshape(-1, 1)) #(-1, 1)\n",
    "    \n",
    "    return hypo_y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#確認用\n",
    "def linear_hypothesis(X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hypo_theta：次の形のndarray, shape (n_samples, 1) Q.この形にする意味は？？\n",
    "     線形の仮定関数による推定結果\n",
    "     予測値が返っている\n",
    "    \"\"\"\n",
    "    #左列に1の列追加(切片用)\n",
    "    one = np.ones(X.shape[0]).reshape(-1, 1)\n",
    "    X = np.concatenate((one, X), axis=1)\n",
    "    \n",
    "    #θ：重み（初期値：0以上1未満の乱数）\n",
    "    coef_ = np.random.rand(X.shape[1])\n",
    "    \n",
    "    #線形回帰の仮定関数\n",
    "    hypo_y = np.dot(X, coef_.reshape(-1, 1)) #(-1, 1)\n",
    "    \n",
    "    return hypo_y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37018898, 0.0375793 ])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_ = np.random.rand(X.shape[1])\n",
    "coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11191833],\n",
       "       [0.04379995]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_ = np.random.rand(X.shape[1], 1)\n",
    "coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2377156 ],\n",
       "       [0.15746902]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_ = np.random.rand(X.shape[1]).reshape(-1, 1)\n",
    "coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】最急降下法\n",
    "最急降下法により学習させる実装を行なってください。  \n",
    "以下の式で表されるパラメータの更新式のメソッドを追加し、fitメソッドから呼び出すようにしてください。\n",
    "\n",
    "α: 学習率\n",
    "\n",
    "\n",
    "i: サンプルのインデックス\n",
    "\n",
    "\n",
    "j: 特徴量のインデックス\n",
    "\n",
    "$$\n",
    "θ_j := θ_j - α\\frac{1}{m}\\sum_{i=1}^{m}[(h_θ(x^{(i)}) - y^{(i)})x_j^{(i)}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最新版\n",
    "def _gradient_descent(self, X, y):\n",
    "    \"\"\"\n",
    "    「サンプル数、学習率、予測との誤差」から、新しい重みを算出\n",
    "    Parameters\n",
    "    --------\n",
    "    X：次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データ\n",
    "    y：目的変数のndarray(n_features, )\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    new_coef_：次の形のndarray, shape (1, n_features)\n",
    "    \"\"\"\n",
    "    hypo_y, X = self._linear_hypothesis(X)\n",
    "    m = X.shape[0] #サンプル数\n",
    "    \n",
    "    error = hypo_y - y.reshape(-1, 1) #誤差の算出 shape(m, 1)\n",
    "    \n",
    "    new_coef_ = self.coef_ - self.lr*(1/m)*np.dot(error.reshape(-1,), X)\n",
    "        \n",
    "    return new_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#不具合有り。これを用いると目的関数の値がどんどん大きくなってしまう。\n",
    "def _gradient_descent(self, X, y):\n",
    "    \"\"\"\n",
    "    「サンプル数、学習率、予測との誤差」から、新しい重みを算出\n",
    "    Parameters\n",
    "    --------\n",
    "    X：次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データ\n",
    "    y：目的変数のndarray(n_features, )\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    new_coef_：次の形のndarray, shape (1, n_features)\n",
    "    \"\"\"\n",
    "    hypo_y, X = self._linear_hypothesis(X)\n",
    "    m = X.shape[0] #サンプル数\n",
    "    \n",
    "    error = hypo_y - y.reshape(-1, 1) #誤差の算出 shape(m, 1)\n",
    "    \n",
    "    new_coef_ = self.coef_ - self.lr*(1/m)*np.dot(error.reshape(-1,), X)\n",
    "        \n",
    "    return new_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#確認用\n",
    "def gradient_descent(X, y):\n",
    "    \"\"\"\n",
    "    「サンプル数、学習率、予測との誤差」から、新しい重みを算出\n",
    "    Parameters\n",
    "    --------\n",
    "    X：次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データ\n",
    "    y：目的変数のndarray(n_features, )\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    new_coef_：次の形のndarray, shape (1, n_features)\n",
    "    \"\"\"\n",
    "    hypo_y, X = linear_hypothesis(X)\n",
    "    m = X.shape[0] #サンプル数\n",
    "    \n",
    "    error = hypo_y - y.reshape(-1, 1) #誤差の算出 shape(m, 1)\n",
    "    \n",
    "    new_coef_ = coef_ - (0.03*(1/m)*np.dot(error.reshape(-1,), X))\n",
    "        \n",
    "    return new_coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】推定\n",
    "推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "\n",
    "仮定関数の出力が推定結果です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】平均二乗誤差\n",
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
    "\n",
    "\n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n",
    "\n",
    "平均二乗誤差は以下の数式で表されます。\n",
    "\n",
    "$$\n",
    "L(θ) = \\frac{1}{m}\\sum_{i=1}^{m}(h_θ(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "$m$ : 入力されるデータの数\n",
    "\n",
    "\n",
    "$h_\\theta()$ : 仮定関数\n",
    "\n",
    "\n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "\n",
    "$y^{(i)}$ : i番目のサンプルの正解値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    m = len(y) #サンプル数\n",
    "    mse = (1/m)*np.sum(np.square(y_pred - y)) #float\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】目的関数\n",
    "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
    "\n",
    "$$\n",
    "J(θ) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_θ(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "$m$ : 入力されるデータの数\n",
    "\n",
    "\n",
    "$h_\\theta()$ : 仮定関数\n",
    "\n",
    "\n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
    "\n",
    "\n",
    "$y^{(i)}$ : i番目のサンプルの正解値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(X, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      サンプル\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    loss : numpy.float\n",
    "      平均二乗誤差/2\n",
    "    \"\"\"\n",
    "    m = len(y) #サンプルサイズ\n",
    "    y_pred, X = self._linear_hypothesis(X)\n",
    "    \n",
    "    mse = MSE(y_pred, y.reshape(-1, 1)) #平均二乗誤差の計算\n",
    "    \n",
    "    loss = mse/2 #float\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#確認用\n",
    "def objective_func2(X, y):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      サンプル\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    loss : numpy.float\n",
    "      平均二乗誤差/2\n",
    "    \"\"\"\n",
    "    m = len(y) #サンプルサイズ\n",
    "    y_pred, X = linear_hypothesis(X)\n",
    "    \n",
    "    loss = 1/(2*m)*np.sum(np.square(y_pred - y.reshape(-1, 1))) #float\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.array([2, 3]).reshape(-1, 1)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [9]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.square(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n",
    "\n",
    "\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]].values\n",
    "y = df.loc[:, \"SalePrice\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1710, 2003],\n",
       "       [1262, 1976],\n",
       "       [1786, 2001],\n",
       "       ...,\n",
       "       [2340, 1941],\n",
       "       [1078, 1950],\n",
       "       [1256, 1965]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([208500, 181500, 223500, ..., 266500, 142125, 147500])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.41376667e+03 9.09869551e+06 1.07101017e+07]\n",
      "[-1.04807218e+09 -1.66646786e+12 -2.06709774e+12]\n",
      "[1.98198498e+14 3.14711888e+17 3.90899931e+17]\n",
      "[-3.74610143e+19 -5.94808362e+22 -7.38830217e+22]\n",
      "[7.08031844e+24 1.12421644e+28 1.39642593e+28]\n",
      "[-1.33821501e+30 -2.12482431e+33 -2.63931368e+33]\n",
      "[2.52929216e+35 4.01602241e+38 4.98843263e+38]\n",
      "[-4.78048654e+40 -7.59047981e+43 -9.42838293e+43]\n",
      "[9.03535460e+45 1.43463800e+49 1.78201073e+49]\n",
      "[-1.70772644e+51 -2.71153636e+54 -3.36808790e+54]\n",
      "train_loss:[1.94514908e+010 6.21691925e+020 2.22082145e+031 7.93340894e+041\n",
      " 2.83404045e+052 1.01240026e+063 3.61658312e+073 1.29194687e+084\n",
      " 4.61520353e+094 1.64868263e+105]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "model = ScratchLinearRegression(num_iter=10, lr=0.03, no_bias=False, verbose=True)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = np.array([1, 2]).reshape(-1, 1)\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = np.array([1, 2, 3, 4, 5, 6]).reshape(2, 3)\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 8, 10, 12]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa*bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1710, 2003],\n",
       "       [1262, 1976],\n",
       "       [1786, 2001],\n",
       "       ...,\n",
       "       [2340, 1941],\n",
       "       [1078, 1950],\n",
       "       [1256, 1965]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1530, 2005],\n",
       "       [1922, 2003],\n",
       "       [1442, 1958],\n",
       "       ...,\n",
       "       [1576, 2006],\n",
       "       [1362, 1993],\n",
       "       [1983, 1999]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
