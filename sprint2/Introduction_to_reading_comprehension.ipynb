{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sprintの目的\n",
    "- 機械学習分野の論文から有益な情報を引き出せるようにする\n",
    "- これまで扱ってきた領域の論文から新たな知識を得る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文読解\n",
    "\n",
    "以下の論文を読み問題に答えてください。CNNを使った物体検出（Object Detection）の代表的な研究です。\n",
    "\n",
    "\n",
    "[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1506.01497.pdf\n",
    "\n",
    "### 問題\n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    "\n",
    "\n",
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "\n",
    "(4) RPNとは何か。\n",
    "\n",
    "\n",
    "(5) RoIプーリングとは何か。\n",
    "\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n",
    "\n",
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。\n",
    "\n",
    "\n",
    "#### 条件\n",
    "- 答える際は論文のどの部分からそれが分かるかを書く。\n",
    "- 必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。\n",
    "- 論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    ">- スーパーピクセルのグループ化に基づく方法  \n",
    "    (e.g.,Selective Search [4], CPMC [22], MCG [23])\n",
    ">- スライディングウィンドウに基づく方法  \n",
    "     (e.g., objectness in windows[24], EdgeBoxes [6])\n",
    ">>2 RELATED WORK  \n",
    ">>There is a large literature on object\n",
    "proposal methods. Comprehensive surveys and comparisons of object proposal methods can be found in\n",
    "[19], [20], [21]. Widely used object proposal methods\n",
    "include those based on grouping super-pixels (e.g.,\n",
    "Selective Search [4], CPMC [22], MCG [23]) and those\n",
    "based on sliding windows (e.g., objectness in windows\n",
    "[24], EdgeBoxes [6]). Object proposal methods were\n",
    "adopted as external modules independent of the detectors (e.g., Selective Search [4] object detectors, RCNN [5], and Fast R-CNN [2]).\n",
    "\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    ">物体検出ネットワーク（領域ベースのもの）で使用される畳み込み特徴マップを、領域提案ネットワークにも共有して使用することで計算コストを下げる仕組み\n",
    ">>1 INTRODUCTION  \n",
    ">> To this end, we\n",
    "introduce novel Region Proposal Networks (RPNs) that\n",
    "share convolutional layers with state-of-the-art object\n",
    "detection networks [1], [2]. By sharing convolutions at\n",
    "test-time, the marginal cost for computing proposals\n",
    "is small (e.g., 10ms per image).  \n",
    "Our observation is that the convolutional feature\n",
    "maps used by region-based detectors, like Fast RCNN, can also be used for generating region proposals. \n",
    ">>\n",
    ">>5 CONCLUSION  \n",
    ">>By sharing convolutional features with the down-stream detection network, the\n",
    "region proposal step is nearly cost-free.\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    ">- One-Stage : 領域提案と領域分類を同時に行う\n",
    ">- Two-Stage : 領域提案後に領域分類をする。2段階。\n",
    ">>4.1 Experiments on PASCAL VOC  \n",
    ">>One-Stage Detection vs. Two-Stage Proposal + De- tection. The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic pro- posals and class-specific detections. In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. These features are used to simultaneously determine the location and category of objects. In RPN, the features are from square (3×3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios. Though both methods use sliding windows, the region proposal task is only the first stage of Faster R- CNN—the downstream Fast R-CNN detector attends to the proposals to refine them. In the second stage of our cascade, the region-wise features are adaptively pooled [1], [2] from proposal boxes that more faith- fully cover the features of the regions. We believe these features lead to more accurate detections.\n",
    "\n",
    "(4) RPNとは何か。\n",
    ">- 地域提案ネットワーク（画像のある部分が、背景なのか物体なのかの識別）\n",
    ">- 様々なスケールとアスペクト比で領域を予測する\n",
    ">>1 INTRODUCTION  \n",
    ">>RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios.  \n",
    "\n",
    ">画像を入力として受け取り、オブジェクトネススコアをもつ長方形のセットを出力。  \n",
    ">>3.1 Region Proposal Networks  \n",
    ">>A Region Proposal Network (RPN) takes an image\n",
    "(of any size) as input and outputs a set of rectangular\n",
    "3 object proposals, each with an objectness score. We\n",
    "model this process with a fully convolutional network [7], which we describe in this section. Because our ulti- mate goal is to share computation with a Fast R-CNN object detection network [2], we assume that both nets share a common set of convolutional layers. In our ex- periments, we investigate the Zeiler and Fergus model [32] (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model [3] (VGG-16), which has 13 shareable convolutional layers.\n",
    "\n",
    "\n",
    "(5) RoIプーリングとは何か。\n",
    ">RoIプーリングレイヤーは、最大プーリングを使用して、有効な関心領域内の特徴を、固定された空間範囲を持つ小さな特徴マップに変換する。\n",
    ">>[Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf)  \n",
    ">>2.1. The RoI pooling layer  \n",
    ">>The RoI pooling layer uses max pooling to convert the\n",
    "features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g., 7 × 7),\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。\n",
    ">複数のサイズのアンカーを使用することが効果的。  \n",
    ">システムを柔軟に保つために、スケールとアスペクト比を設計に採用している。  \n",
    ">>4.1 Experiments on PASCAL VOC  \n",
    ">>**Sensitivities to Hyper-parameters.** In Table 8 we investigate the settings of anchors. By default we use 3 scales and 3 aspect ratios (69.9% mAP in Table 8). If using just one anchor at each position, the mAP drops by a considerable margin of 3-4%. The mAP is higher if using 3 scales (with 1 aspect ratio) or 3 aspect ratios (with 1 scale), demonstrating that using anchors of multiple sizes as the regression references is an effective solution. Using just 3 scales with 1 aspect ratio (69.8%) is as good as using 3 scales with 3 aspect ratios on this dataset, suggesting that scales and aspect ratios are not disentangled dimensions for the detection accuracy. But we still adopt these two dimensions in our designs to keep our system flexible.\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    ">データセット：PASCAL VOC 2007, Microsoft COCO  \n",
    ">指標値：  \n",
    ">mAP---58.7%,58.6% → 59.9%  \n",
    ">mAP @0.5---39.3% → 42.1%\n",
    ">>4 EXPERIMENTS  \n",
    "\n",
    ">PASCAL VOC 2007はFast R-CNNの実験でも使用されたデータセットである。  \n",
    ">>[Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf)  \n",
    "\n",
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
