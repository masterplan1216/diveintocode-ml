{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sprintの目的\n",
    "- 自然言語処理の一連の流れを学ぶ\n",
    "- 自然言語のベクトル化の方法を学ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自然言語処理により何ができるか\n",
    "機械学習の入力や出力に自然言語のテキストを用いることで様々なことができます。入力も出力もテキストである例としては 機械翻訳 があげられ、実用化されています。入力は画像で出力がテキストである 画像キャプション生成 やその逆の文章からの画像生成も研究が進んでいます。\n",
    "\n",
    "\n",
    "しかし、出力をテキストや画像のような非構造化データとすることは難易度が高いです。比較的簡単にできることとしては、入力をテキスト、出力をカテゴリーとする テキスト分類 です。\n",
    "\n",
    "\n",
    "アヤメやタイタニック、手書き数字のような定番の存在として、IMDB映画レビューデータセット の感情分析があります。レビューの文書が映画に対して肯定的か否定的かを2値分類します。文書ごとの肯定・否定はラベルが与えられています。このSprintではこれを使っていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB映画レビューデータセットの準備\n",
    "\n",
    "IMDB映画レビューデータセットを準備します。\n",
    "\n",
    "\n",
    "次のwgetコマンドによってダウンロードします。\n",
    "\n",
    "[Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wgetコマンドのインストール\n",
    "\n",
    "#「wget」コマンドは“ノンインタラクティブなダウンローダー”です。\n",
    "#「wget URL」で指定したURLのファイルをダウンロードします。\n",
    "\n",
    "#!brew install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\r\n",
      "\r\n",
      "Overview\r\n",
      "\r\n",
      "This dataset contains movie reviews along with their associated binary\r\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\r\n",
      "sentiment classification. This document outlines how the dataset was\r\n",
      "gathered, and how to use the files provided. \r\n",
      "\r\n",
      "Dataset \r\n",
      "\r\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\r\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\r\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\r\n",
      "documents for unsupervised learning. \r\n",
      "\r\n",
      "In the entire collection, no more than 30 reviews are allowed for any\r\n",
      "given movie because reviews for the same movie tend to have correlated\r\n",
      "ratings. Further, the train and test sets contain a disjoint set of\r\n",
      "movies, so no significant performance is obtained by memorizing\r\n",
      "movie-unique terms and their associated with observed labels.  In the\r\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\r\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\r\n",
      "more neutral ratings are not included in the train/test sets. In the\r\n",
      "unsupervised set, reviews of any rating are included and there are an\r\n",
      "even number of reviews > 5 and <= 5.\r\n",
      "\r\n",
      "Files\r\n",
      "\r\n",
      "There are two top-level directories [train/, test/] corresponding to\r\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\r\n",
      "the reviews with binary labels positive and negative. Within these\r\n",
      "directories, reviews are stored in text files named following the\r\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\r\n",
      "the star rating for that review on a 1-10 scale. For example, the file\r\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\r\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\r\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\r\n",
      "omitted for this portion of the dataset.\r\n",
      "\r\n",
      "We also include the IMDb URLs for each review in a separate\r\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\r\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\r\n",
      "are unable to link directly to the review, but only to the movie's\r\n",
      "review page.\r\n",
      "\r\n",
      "In addition to the review text files, we include already-tokenized bag\r\n",
      "of words (BoW) features that were used in our experiments. These \r\n",
      "are stored in .feat files in the train/test directories. Each .feat\r\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\r\n",
      "data.  The feature indices in these files start from 0, and the text\r\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\r\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\r\n",
      "(the) appears 7 times in that review.\r\n",
      "\r\n",
      "LIBSVM page for details on .feat file format:\r\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\r\n",
      "\r\n",
      "We also include [imdbEr.txt] which contains the expected rating for\r\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\r\n",
      "rating is a good way to get a sense for the average polarity of a word\r\n",
      "in the dataset.\r\n",
      "\r\n",
      "Citing the dataset\r\n",
      "\r\n",
      "When using this dataset please cite our ACL 2011 paper which\r\n",
      "introduces it. This paper also contains classification results which\r\n",
      "you may want to compare against.\r\n",
      "\r\n",
      "\r\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\r\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\r\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\r\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\r\n",
      "  month     = {June},\r\n",
      "  year      = {2011},\r\n",
      "  address   = {Portland, Oregon, USA},\r\n",
      "  publisher = {Association for Computational Linguistics},\r\n",
      "  pages     = {142--150},\r\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\r\n",
      "}\r\n",
      "\r\n",
      "References\r\n",
      "\r\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\r\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\r\n",
      "636-659.\r\n",
      "\r\n",
      "Contact\r\n",
      "\r\n",
      "For questions/comments/corrections please contact Andrew Maas\r\n",
      "amaas@cs.stanford.edu\r\n"
     ]
    }
   ],
   "source": [
    "# IMDBをカレントフォルダにダウンロード\n",
    "#!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# 解凍\n",
    "#!tar zxf aclImdb_v1.tar.gz\n",
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "#!rm -rf aclImdb/train/unsup\n",
    "# IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learnのload_filesを用いて読み込みます。\n",
    "\n",
    "\n",
    "[sklearn.datasets.load_files — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n",
      "-------------------------\n",
      "[1 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(\"-------------------------\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x_trainが文章データ、y_trainがラベル**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDBはInternet Movie Databaseの略で、映画のデータベースサイトです。\n",
    "\n",
    "\n",
    "[Ratings and Reviews for New Movies and TV Shows - IMDb](https://www.imdb.com/)\n",
    "\n",
    "\n",
    "このサイトではユーザーが映画に対して1から10点の評価とコメントを投稿することができます。そのデータベースから訓練データは25000件、テストデータは25000件のデータセットを作成しています。\n",
    "\n",
    "\n",
    "4点以下を否定的、7点以下を肯定的なレビューとして2値のラベル付けしており、これにより感情の分類を行います。5,6点の中立的なレビューはデータセットに含んでいません。また、ラベルは訓練用・テスト用それぞれで均一に入っています。詳細はダウンロードしたREADMEを確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 古典的な手法\n",
    "\n",
    "古典的ながら現在でも強力な手法であるBoWとTF-IDFを見ていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW\n",
    "\n",
    "単純ながら効果的な方法として BoW (Bag of Words) があります。これは、サンプルごとに単語などの 登場回数 を数えたものをベクトルとする方法です。単語をカテゴリとして捉え one-hot表現 していることになります。\n",
    "\n",
    "例として、IMDBデータセットからある3文の最初の5単語を抜き出したものを用意しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is very good.\",\n",
    "  \"This film is a good\",\n",
    "  \"Very bad. Very, very bad.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この3文にBoWを適用させてみます。scikit-learnのCountVectorizerを利用します。\n",
    "\n",
    "[sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  is  movie  this  very\n",
       "0  0    0     0     1   1      1     1     1\n",
       "1  1    0     1     1   1      0     1     0\n",
       "2  0    2     0     0   0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#文章中に出てくる単語について何回使用されたかをカウント\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "# DataFrameにまとめる\n",
    "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例にあげた3文の中で登場する8種類の単語が列名になり、0,1,2番目のサンプルでそれらが何回登場しているかを示しています。2番目のサンプル「Very bad. Very, very bad.」ではbadが2回、veryが3回登場しています。列名になっている言葉はデータセットが持つ 語彙 と呼びます。\n",
    "\n",
    "\n",
    "テキストはBoWにより各サンプルが語彙数の次元を持つ特徴量となり、機械学習モデルへ入力できるようになります。この時使用したテキスト全体のことを コーパス と呼びます。語彙はコーパスに含まれる言葉よって決まり、それを特徴量としてモデルの学習を行います。そのため、テストデータではじめて登場する語彙はベクトル化される際に無視されます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前処理\n",
    "CountVectorizerクラスでは大文字は小文字に揃えるという 前処理 が自動的に行われています。こういった前処理は自然言語処理において大切で、不要な記号などの消去（テキストクリーニング）や表記揺れの統一といったことを別途行うことが一般的です。\n",
    "\n",
    "\n",
    "語形が「see」「saw」「seen」のように変化する単語に対して語幹に揃える ステミング と呼ばれる処理を行うこともあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### トークン\n",
    "BoWは厳密には単語を数えているのではなく、 トークン（token） として定めた固まりを数えます。\n",
    "\n",
    "\n",
    "何をトークンとするかはCountVectorizerでは引数token_patternで 正規表現 の記法により指定されます。デフォルトはr'(?u)\\b\\w\\w+\\b'ですが、上の例ではr'(?u)\\b\\w+\\b'としています。\n",
    "\n",
    "\n",
    "デフォルトでは空白・句読点・スラッシュなどに囲まれた2文字以上の文字を1つのトークンとして抜き出すようになっているため、「a」や「I」などがカウントされません。英語では1文字の単語は文章の特徴をあまり表さないため、除外されることもあります。しかし、上の例では1文字の単語もトークンとして抜き出すように引数を指定しています。\n",
    "\n",
    "\n",
    "《正規表現》\n",
    "\n",
    "\n",
    "正規表現は前処理の際にも活用しますが、ここでは詳細は扱いません。Pythonではreモジュールによって正規表現操作ができます。\n",
    "\n",
    "\n",
    "[re — 正規表現操作](https://docs.python.org/ja/3/library/re.html)\n",
    "\n",
    "\n",
    "正規表現を利用する際はリアルタイムで結果を確認できる以下のようなサービスが便利です。\n",
    "\n",
    "\n",
    "[Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript](https://regex101.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正規表現　＝　正規表現とは、文字列を1つのパターン化した文字列で表現する表記法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 形態素解析\n",
    "英語などの多くの言語では空白という分かりやすい基準でトークン化が行えますが、日本語ではそれが行えません。\n",
    "\n",
    "\n",
    "日本語では名詞や助詞、動詞のように異なる 品詞 で分けられる単位で 分かち書き することになります。例えば「私はプログラミングを学びます」という日本語の文は「私/は/プログラミング/を/学び/ます」という風になります。\n",
    "\n",
    "\n",
    "これには MeCab や Janome のような形態素解析ツールを用います。Pythonから利用することも可能です。MeCabをウェブ上で簡単に利用できるWeb茶まめというサービスも国立国語研究所が提供しています。\n",
    "\n",
    "\n",
    "自然言語では新しい言葉も日々生まれますので、それにどれだけ対応できるかも大切です。MeCab用の毎週更新される辞書として mecab-ipadic-NEologd がオープンソースで存在しています。\n",
    "\n",
    "\n",
    "[mecab-ipadic-neologd/README.ja.md at master · neologd/mecab-ipadic-neologd](https://github.com/neologd/mecab-ipadic-neologd/blob/master/README.ja.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-gram\n",
    "上のBoWの例では1つの単語（トークン）毎の登場回数を数えましたが、これでは語順は全く考慮されていません。\n",
    "\n",
    "\n",
    "考慮するために、隣あう単語同士をまとめて扱う n-gram という考え方を適用することがあります。2つの単語をまとめる場合は 2-gram (bigram) と呼び、次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is very good.\",\n",
    "  \"This film is a good\",\n",
    "  \"Very bad. Very, very bad.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a good</th>\n",
       "      <th>bad very</th>\n",
       "      <th>film is</th>\n",
       "      <th>is a</th>\n",
       "      <th>is very</th>\n",
       "      <th>movie is</th>\n",
       "      <th>this film</th>\n",
       "      <th>this movie</th>\n",
       "      <th>very bad</th>\n",
       "      <th>very good</th>\n",
       "      <th>very very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a good  bad very  film is  is a  is very  movie is  this film  this movie  \\\n",
       "0       0         0        0     0        1         1          0           1   \n",
       "1       1         0        1     1        0         0          1           0   \n",
       "2       0         1        0     0        0         0          0           0   \n",
       "\n",
       "   very bad  very good  very very  \n",
       "0         0          1          0  \n",
       "1         0          0          0  \n",
       "2         2          0          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ngram_rangeで利用するn-gramの範囲を指定する\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-gramにより「very good」と「very bad」が区別して数えられています。\n",
    "\n",
    "\n",
    "単語をまとめない場合は 1-gram (unigram) と呼びます。3つまとめる3-gram(trigram)など任意の数を考えることができます。1-gramと2-gramを組み合わせてBoWを行うといったこともあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題1】BoWのスクラッチ実装\n",
    "以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。\n",
    "\n",
    "This movie is SOOOO funny!!!  \n",
    "What a movie! I never  \n",
    "best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \\\n",
    "  [\"This movie is SOOOO funny!!!\",\n",
    "  \"What a movie! I never\",\n",
    "  \"best movie ever!!!!! this movie\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-gramの場合**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>soooo</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this  movie  is  soooo  funny  what  a  i  never  best  ever\n",
       "0     1      1   1      1      1     0  0  0      0     0     0\n",
       "1     0      1   0      0      0     1  1  1      1     0     0\n",
       "2     1      2   0      0      0     0  0  0      0     1     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#行データのみのデータフレーム作成\n",
    "df = pd.DataFrame(index=range(len(corpus)))\n",
    "display(df)\n",
    "print(\"---------------\")\n",
    "\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    text = text.lower() #小文字に\n",
    "    text = text.replace(\"!\", \"\") #！消す\n",
    "    words = text.split(\" \") #分割 & リスト化\n",
    "    for word in words:\n",
    "        if word not in df.columns:\n",
    "            df[word] = 0\n",
    "        df.loc[i, word] += 1\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-gramの場合**  \n",
    "[Pythonで文字列を連結・結合（+演算子、joinなど）](https://note.nkmk.me/python-string-concat/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>is soooo</th>\n",
       "      <th>soooo funny</th>\n",
       "      <th>what a</th>\n",
       "      <th>a movie</th>\n",
       "      <th>movie i</th>\n",
       "      <th>i never</th>\n",
       "      <th>best movie</th>\n",
       "      <th>movie ever</th>\n",
       "      <th>ever this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this movie  movie is  is soooo  soooo funny  what a  a movie  movie i  \\\n",
       "0           1         1         1            1       0        0        0   \n",
       "1           0         0         0            0       1        1        1   \n",
       "2           1         0         0            0       0        0        0   \n",
       "\n",
       "   i never  best movie  movie ever  ever this  \n",
       "0        0           0           0          0  \n",
       "1        1           0           0          0  \n",
       "2        0           1           1          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_gram = 2\n",
    "\n",
    "#行データのみのデータフレーム作成\n",
    "df = pd.DataFrame(index=range(len(corpus)))\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    join_words_list = [] #結合した単語の保存用リスト\n",
    "    \n",
    "    text = text.lower() #小文字に\n",
    "    text = text.replace(\"!\", \"\") #！消す\n",
    "    words = text.split(\" \") #分割 & リスト化\n",
    "    for j in range(len(words) - 1):\n",
    "        join_words = \" \".join(words[j : j+n_gram])\n",
    "        join_words_list.append(join_words)\n",
    "        \n",
    "    for join_word in join_words_list:\n",
    "        if join_word not in df.columns:\n",
    "            df[join_word] = 0\n",
    "        df.loc[i, join_word] += 1\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "BoWの発展的手法として TF-IDF もよく使われます。これは Term Frequency (TF) と Inverse Document Frequency (IDF) という2つの指標の組み合わせです。  \n",
    "TF：文章内の単語の総数  \n",
    "IDF：全文章中での単語の出現頻度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF\n",
    "IDFはそのトークンがデータセット内で珍しいほど値が大きくなる指標です。\n",
    "\n",
    "\n",
    "サンプル数 $N$ をIMDB映画レビューデータセットの訓練データに合わせ25000として、トークンが出現するサンプル数 $df(t)$ を変化させたグラフを確認してみると、次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n_samples = 25000\n",
    "idf = np.log(n_samples/np.arange(1,n_samples))\n",
    "plt.title(\"IDF\")\n",
    "plt.xlabel(\"df(t)\")\n",
    "plt.ylabel(\"IDF\")\n",
    "plt.plot(idf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDFではこの数を出現回数に掛け合わせるので、珍しいトークンの登場に重み付けを行なっていることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ストップワード\n",
    "あまりにも頻繁に登場するトークンは、値を小さくするだけでなく、取り除くという前処理を加えることもあります。取り除くもののことを ストップワード と呼びます。既存のストップワード一覧を利用したり、しきい値によって求めたりします。\n",
    "\n",
    "\n",
    "scikit-learnのCountVectorizerでは引数stop_wordsにリストで指定することで処理を行なってくれます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  movie  this  very\n",
       "0  0    0     0     1      1     1     1\n",
       "1  1    0     1     1      0     1     0\n",
       "2  0    2     0     0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代表的な既存のストップワード一覧としては、NLTK という自然言語処理のライブラリのものがあげられます。あるデータセットにおいては特別重要な意味を持つ単語が一覧に含まれている可能性もあるため、使用する際は中身を確認することが望ましいです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/itonaoki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# はじめて使う場合はストップワードをダウンロード\n",
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆に、登場回数が特に少ないトークンも取り除くことが多いです。全てのトークンを用いるとベクトルの次元数が著しく大きくなってしまい計算コストが高まるためです。\n",
    "\n",
    "\n",
    "scikit-learnのCountVectorizerでは引数max_featuresに最大の語彙数を指定することで処理を行なってくれます。以下の例では出現数が多い順に5個でベクトル化しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  is  this  very\n",
       "0    0     1   1     1     1\n",
       "1    0     1   1     1     0\n",
       "2    2     0   0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題2】TF-IDFの計算\n",
    "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n",
    "\n",
    "\n",
    "TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。\n",
    "\n",
    "\n",
    "[sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "[sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
    "\n",
    "\n",
    "なお、scikit-learnでは標準的な式とは異なる式が採用されています。\n",
    "\n",
    "\n",
    "また、デフォルトではnorm=\"l2\"の引数が設定されており、各サンプルにL2正規化が行われます。norm=Noneとすることで正規化は行われなくなります。\n",
    "\n",
    "Term Frequency:\n",
    "$$\n",
    "tf(t, d) = n_{t, d}\n",
    "$$\n",
    "\n",
    "$n_{t, d}$ : サンプルd内のトークンtの出現回数\n",
    "\n",
    "\n",
    "scikit-learnのTFは分母がなくなりBoWと同じ計算になります。\n",
    "\n",
    "Inverse Document Frequency:\n",
    "$$\n",
    "idf(t) = log\\frac{1 + N}{1 + df(t)} + 1\n",
    "$$\n",
    "\n",
    "$N$ : サンプル数\n",
    "\n",
    "$df(t)$ : トークンtが出現するサンプル数\n",
    "\n",
    "※logの底はネイピア数e\n",
    "\n",
    "[5.2.3.4. Tf–idf term weighting — scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>13th</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>z</th>\n",
       "      <th>zellweger</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoey</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057319</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154575</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.120631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   00  000         1        10       100        11        12   13  \\\n",
       "0      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "2      0.0  0.0  0.0  0.000000  0.124442  0.000000  0.000000  0.000000  0.0   \n",
       "3      0.0  0.0  0.0  0.000000  0.085006  0.000000  0.000000  0.000000  0.0   \n",
       "4      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "5      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "6      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "7      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "8      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "9      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "10     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "11     0.0  0.0  0.0  0.000000  0.077387  0.000000  0.000000  0.000000  0.0   \n",
       "12     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "13     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "14     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "15     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "16     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "17     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "18     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "19     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "20     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "21     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "22     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "23     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "25     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "26     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "27     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "28     0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.264061  0.0   \n",
       "29     0.0  0.0  0.0  0.000000  0.063400  0.000000  0.000000  0.000000  0.0   \n",
       "...    ...  ...  ...       ...       ...       ...       ...       ...  ...   \n",
       "24970  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24971  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24972  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24973  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24974  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24975  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24976  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24977  0.0  0.0  0.0  0.000000  0.048106  0.000000  0.000000  0.000000  0.0   \n",
       "24978  0.0  0.0  0.0  0.000000  0.074542  0.000000  0.000000  0.000000  0.0   \n",
       "24979  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24980  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24981  0.0  0.0  0.0  0.000000  0.022933  0.000000  0.000000  0.000000  0.0   \n",
       "24982  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24983  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24984  0.0  0.0  0.0  0.055112  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24985  0.0  0.0  0.0  0.000000  0.044734  0.000000  0.000000  0.000000  0.0   \n",
       "24986  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24987  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24988  0.0  0.0  0.0  0.000000  0.057791  0.000000  0.000000  0.000000  0.0   \n",
       "24989  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24990  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24991  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24992  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.072277  0.000000  0.0   \n",
       "24993  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24994  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24995  0.0  0.0  0.0  0.000000  0.000000  0.052273  0.000000  0.000000  0.0   \n",
       "24996  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24997  0.0  0.0  0.0  0.000000  0.068246  0.000000  0.000000  0.000000  0.0   \n",
       "24998  0.0  0.0  0.0  0.101521  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "24999  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "\n",
       "       13th  ...      young   younger  youth         z  zellweger      zero  \\\n",
       "0       0.0  ...   0.000000  0.164505    0.0  0.000000        0.0  0.136932   \n",
       "1       0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "2       0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "3       0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "4       0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "5       0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "6       0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "7       0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "8       0.0  ...   0.000000  0.035996    0.0  0.000000        0.0  0.000000   \n",
       "9       0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "10      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "11      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "12      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "13      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "14      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "15      0.0  ...   0.000000  0.055622    0.0  0.000000        0.0  0.000000   \n",
       "16      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "17      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "18      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "19      0.0  ...   0.000000  0.075760    0.0  0.000000        0.0  0.000000   \n",
       "20      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "21      0.0  ...   0.000000  0.026512    0.0  0.092406        0.0  0.000000   \n",
       "22      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "23      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "25      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "26      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "27      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.120631   \n",
       "28      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "29      0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "...     ...  ...        ...       ...    ...       ...        ...       ...   \n",
       "24970   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24971   0.0  ...   0.000000  0.064679    0.0  0.000000        0.0  0.000000   \n",
       "24972   0.0  ...   0.000000  0.082719    0.0  0.000000        0.0  0.000000   \n",
       "24973   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24974   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24975   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24976   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24977   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24978   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24979   0.0  ...   0.000000  0.056388    0.0  0.000000        0.0  0.000000   \n",
       "24980   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.282172   \n",
       "24981   0.0  ...   0.000000  0.024615    0.0  0.000000        0.0  0.000000   \n",
       "24982   0.0  ...   0.000000  0.075132    0.0  0.000000        0.0  0.000000   \n",
       "24983   0.0  ...   0.000000  0.127050    0.0  0.000000        0.0  0.000000   \n",
       "24984   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24985   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24986   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24987   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24988   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24989   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24990   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24991   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24992   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24993   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24994   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24995   0.0  ...   0.095763  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24996   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24997   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24998   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "24999   0.0  ...   0.000000  0.000000    0.0  0.000000        0.0  0.000000   \n",
       "\n",
       "       zoey  zombie   zombies  zone  \n",
       "0       0.0     0.0  0.000000   0.0  \n",
       "1       0.0     0.0  0.000000   0.0  \n",
       "2       0.0     0.0  0.000000   0.0  \n",
       "3       0.0     0.0  0.000000   0.0  \n",
       "4       0.0     0.0  0.000000   0.0  \n",
       "5       0.0     0.0  0.000000   0.0  \n",
       "6       0.0     0.0  0.000000   0.0  \n",
       "7       0.0     0.0  0.000000   0.0  \n",
       "8       0.0     0.0  0.000000   0.0  \n",
       "9       0.0     0.0  0.000000   0.0  \n",
       "10      0.0     0.0  0.000000   0.0  \n",
       "11      0.0     0.0  0.000000   0.0  \n",
       "12      0.0     0.0  0.000000   0.0  \n",
       "13      0.0     0.0  0.000000   0.0  \n",
       "14      0.0     0.0  0.000000   0.0  \n",
       "15      0.0     0.0  0.000000   0.0  \n",
       "16      0.0     0.0  0.000000   0.0  \n",
       "17      0.0     0.0  0.057319   0.0  \n",
       "18      0.0     0.0  0.000000   0.0  \n",
       "19      0.0     0.0  0.000000   0.0  \n",
       "20      0.0     0.0  0.000000   0.0  \n",
       "21      0.0     0.0  0.000000   0.0  \n",
       "22      0.0     0.0  0.000000   0.0  \n",
       "23      0.0     0.0  0.000000   0.0  \n",
       "24      0.0     0.0  0.000000   0.0  \n",
       "25      0.0     0.0  0.154575   0.0  \n",
       "26      0.0     0.0  0.000000   0.0  \n",
       "27      0.0     0.0  0.000000   0.0  \n",
       "28      0.0     0.0  0.000000   0.0  \n",
       "29      0.0     0.0  0.000000   0.0  \n",
       "...     ...     ...       ...   ...  \n",
       "24970   0.0     0.0  0.000000   0.0  \n",
       "24971   0.0     0.0  0.000000   0.0  \n",
       "24972   0.0     0.0  0.000000   0.0  \n",
       "24973   0.0     0.0  0.000000   0.0  \n",
       "24974   0.0     0.0  0.000000   0.0  \n",
       "24975   0.0     0.0  0.000000   0.0  \n",
       "24976   0.0     0.0  0.000000   0.0  \n",
       "24977   0.0     0.0  0.000000   0.0  \n",
       "24978   0.0     0.0  0.000000   0.0  \n",
       "24979   0.0     0.0  0.000000   0.0  \n",
       "24980   0.0     0.0  0.000000   0.0  \n",
       "24981   0.0     0.0  0.000000   0.0  \n",
       "24982   0.0     0.0  0.000000   0.0  \n",
       "24983   0.0     0.0  0.000000   0.0  \n",
       "24984   0.0     0.0  0.000000   0.0  \n",
       "24985   0.0     0.0  0.000000   0.0  \n",
       "24986   0.0     0.0  0.000000   0.0  \n",
       "24987   0.0     0.0  0.000000   0.0  \n",
       "24988   0.0     0.0  0.000000   0.0  \n",
       "24989   0.0     0.0  0.000000   0.0  \n",
       "24990   0.0     0.0  0.000000   0.0  \n",
       "24991   0.0     0.0  0.000000   0.0  \n",
       "24992   0.0     0.0  0.000000   0.0  \n",
       "24993   0.0     0.0  0.000000   0.0  \n",
       "24994   0.0     0.0  0.000000   0.0  \n",
       "24995   0.0     0.0  0.000000   0.0  \n",
       "24996   0.0     0.0  0.000000   0.0  \n",
       "24997   0.0     0.0  0.000000   0.0  \n",
       "24998   0.0     0.0  0.000000   0.0  \n",
       "24999   0.0     0.0  0.000000   0.0  \n",
       "\n",
       "[25000 rows x 5000 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=5000, token_pattern=r'(?u)\\b\\w+\\b')\n",
    "X_train = vectorizer.fit_transform(x_train).toarray()\n",
    "X_test = vectorizer.fit_transform(x_test).toarray()\n",
    "cols = vectorizer.get_feature_names()\n",
    "df = pd.DataFrame(X_train, columns=cols)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hate': 2077,\n",
       " 'graham': 1973,\n",
       " 'beautiful': 435,\n",
       " 'fun': 1870,\n",
       " 'watch': 4840,\n",
       " 'movie': 2948,\n",
       " 'like': 2616,\n",
       " 'hip': 2141,\n",
       " 'clothing': 863,\n",
       " 'surroundings': 4361,\n",
       " 'actors': 121,\n",
       " 'flick': 1774,\n",
       " 'work': 4945,\n",
       " 'well': 4870,\n",
       " 'together': 4531,\n",
       " 'casey': 711,\n",
       " 'hysterical': 2232,\n",
       " 'literally': 2637,\n",
       " 'lights': 2614,\n",
       " 'screen': 3879,\n",
       " 'minor': 2877,\n",
       " 'characters': 765,\n",
       " 'patricia': 3232,\n",
       " 'talented': 4397,\n",
       " 'gorgeous': 1962,\n",
       " 'director': 1286,\n",
       " 'lisa': 2631,\n",
       " 'know': 2503,\n",
       " 'received': 3596,\n",
       " 'many': 2747,\n",
       " 'positive': 3373,\n",
       " 'comments': 913,\n",
       " 'one': 3132,\n",
       " 'call': 651,\n",
       " 'artistic': 320,\n",
       " 'beautifully': 436,\n",
       " 'filmed': 1733,\n",
       " 'things': 4481,\n",
       " 'make': 2729,\n",
       " 'empty': 1480,\n",
       " 'plot': 3336,\n",
       " 'filled': 1730,\n",
       " 'sexual': 3954,\n",
       " 'wish': 4915,\n",
       " 'wasted': 4838,\n",
       " 'time': 4520,\n",
       " 'rather': 3565,\n",
       " 'poor': 3354,\n",
       " 'excuse': 1584,\n",
       " 'strange': 4258,\n",
       " 'behavior': 449,\n",
       " 'another': 263,\n",
       " 'hollywood': 2166,\n",
       " 'attempt': 350,\n",
       " 'convince': 1001,\n",
       " 'us': 4718,\n",
       " 'kind': 2491,\n",
       " 'life': 2607,\n",
       " 'normal': 3060,\n",
       " 'ok': 3125,\n",
       " 'beginning': 446,\n",
       " 'asked': 327,\n",
       " 'self': 3917,\n",
       " 'point': 3342,\n",
       " 'continued': 988,\n",
       " 'watching': 4844,\n",
       " 'hoping': 2187,\n",
       " 'would': 4962,\n",
       " 'change': 754,\n",
       " 'quite': 3535,\n",
       " 'disappointed': 1297,\n",
       " 'glad': 1938,\n",
       " 'spend': 4151,\n",
       " 'money': 2910,\n",
       " 'see': 3903,\n",
       " 'theater': 4469,\n",
       " 'caught': 726,\n",
       " 'horror': 2195,\n",
       " 'channel': 758,\n",
       " 'impressed': 2272,\n",
       " 'film': 1732,\n",
       " 'gothic': 1965,\n",
       " 'atmosphere': 343,\n",
       " 'tone': 4537,\n",
       " 'big': 480,\n",
       " 'fan': 1666,\n",
       " 'vampire': 4739,\n",
       " 'related': 3630,\n",
       " 'always': 220,\n",
       " 'happy': 2061,\n",
       " 'new': 3030,\n",
       " 'case': 709,\n",
       " 'creature': 1070,\n",
       " 'dimension': 1275,\n",
       " 'done': 1338,\n",
       " 'brilliant': 583,\n",
       " 'job': 2419,\n",
       " 'dark': 1143,\n",
       " 'mood': 2920,\n",
       " 'subject': 4299,\n",
       " 'using': 4726,\n",
       " 'art': 316,\n",
       " 'scene': 3861,\n",
       " 'backdrop': 387,\n",
       " 'essentially': 1541,\n",
       " 'tale': 4395,\n",
       " 'love': 2686,\n",
       " 'space': 4134,\n",
       " 'pure': 3509,\n",
       " 'friendship': 1858,\n",
       " 'opposed': 3144,\n",
       " 'lust': 2705,\n",
       " 'blood': 514,\n",
       " 'vampires': 4740,\n",
       " 'story': 4252,\n",
       " 'means': 2802,\n",
       " 'mind': 2867,\n",
       " 'substance': 4304,\n",
       " 'shape': 3967,\n",
       " 'appears': 291,\n",
       " 'grant': 1977,\n",
       " 'hearts': 2098,\n",
       " 'whilst': 4885,\n",
       " 'essence': 1539,\n",
       " 'drug': 1389,\n",
       " 'addiction': 135,\n",
       " 'loss': 2677,\n",
       " 'control': 994,\n",
       " 'group': 2003,\n",
       " 'friends': 1857,\n",
       " 'circle': 822,\n",
       " 'enjoyed': 1501,\n",
       " '2': 39,\n",
       " 'main': 2723,\n",
       " 'male': 2735,\n",
       " 'chris': 807,\n",
       " 'alex': 195,\n",
       " 'attractive': 359,\n",
       " 'plus': 3338,\n",
       " 'female': 1708,\n",
       " 'viewers': 4768,\n",
       " 'special': 4143,\n",
       " 'effects': 1448,\n",
       " 'set': 3945,\n",
       " 'design': 1228,\n",
       " 'effective': 1446,\n",
       " 'enjoyable': 1500,\n",
       " 'take': 4391,\n",
       " 'recommended': 3605,\n",
       " 'anyone': 272,\n",
       " 'likes': 2619,\n",
       " 'intelligence': 2331,\n",
       " 'action': 116,\n",
       " 'thing': 4480,\n",
       " 'missing': 2889,\n",
       " 'even': 1551,\n",
       " 'better': 473,\n",
       " 'bit': 490,\n",
       " 'nudity': 3085,\n",
       " 'suited': 4330,\n",
       " 'themes': 4474,\n",
       " 'nbc': 3004,\n",
       " 'chance': 752,\n",
       " 'powerful': 3387,\n",
       " 'religious': 3645,\n",
       " 'epic': 1521,\n",
       " 'along': 211,\n",
       " 'lines': 2627,\n",
       " 'ten': 4444,\n",
       " 'greatest': 1987,\n",
       " 'ever': 1556,\n",
       " 'told': 4533,\n",
       " 'instead': 2326,\n",
       " 'chose': 805,\n",
       " 'cartoon': 707,\n",
       " 'anything': 273,\n",
       " 'else': 1466,\n",
       " 'recall': 3594,\n",
       " 'bible': 478,\n",
       " 'lot': 2679,\n",
       " 'turns': 4633,\n",
       " 'attacks': 349,\n",
       " 'ark': 303,\n",
       " 'remember': 3653,\n",
       " 'noah': 3048,\n",
       " 'son': 4114,\n",
       " 'develops': 1249,\n",
       " 'serious': 3938,\n",
       " 'orange': 3146,\n",
       " 'crazy': 1059,\n",
       " 'old': 3127,\n",
       " 'suddenly': 4318,\n",
       " 'acts': 124,\n",
       " 'runs': 3801,\n",
       " 'around': 309,\n",
       " 'shouting': 3998,\n",
       " 'terms': 4451,\n",
       " 'possibly': 3378,\n",
       " 'worst': 4957,\n",
       " 'marketing': 2758,\n",
       " 'decision': 1180,\n",
       " 'history': 2147,\n",
       " 'obviously': 3100,\n",
       " 'majority': 2728,\n",
       " 'people': 3244,\n",
       " 'going': 1951,\n",
       " 'jewish': 2414,\n",
       " 'christian': 809,\n",
       " 'parents': 3204,\n",
       " 'kids': 2482,\n",
       " 'earth': 1419,\n",
       " 'offensive': 3113,\n",
       " 'trying': 4624,\n",
       " 'way': 4850,\n",
       " 'least': 2570,\n",
       " 'reel': 3613,\n",
       " 'right': 3734,\n",
       " 'audience': 360,\n",
       " 'hope': 2182,\n",
       " 'real': 3579,\n",
       " 'seriously': 3939,\n",
       " 'actually': 126,\n",
       " 'makes': 2732,\n",
       " 'waste': 4837,\n",
       " 'trash': 4584,\n",
       " 'looking': 2667,\n",
       " 'something': 4109,\n",
       " 'shocking': 3985,\n",
       " 'okay': 3126,\n",
       " 'fine': 1745,\n",
       " 'imagery': 2255,\n",
       " 'attempts': 353,\n",
       " 'deep': 1184,\n",
       " 'various': 4743,\n",
       " 'symbolism': 4382,\n",
       " 'ends': 1489,\n",
       " 'annoying': 262,\n",
       " 'sure': 4350,\n",
       " 'purpose': 3511,\n",
       " 'truly': 4620,\n",
       " 'portray': 3367,\n",
       " 'sort': 4121,\n",
       " 'message': 2839,\n",
       " 'shock': 3983,\n",
       " 'hell': 2108,\n",
       " 'gore': 1961,\n",
       " 'sex': 3953,\n",
       " 'violence': 4777,\n",
       " 'thinking': 4483,\n",
       " 'probably': 3442,\n",
       " 'first': 1755,\n",
       " 'failed': 1646,\n",
       " 'simply': 4030,\n",
       " 'ended': 1485,\n",
       " 'piece': 3296,\n",
       " 'garbage': 1887,\n",
       " 'lots': 2680,\n",
       " 'obnoxious': 3095,\n",
       " 'independent': 2292,\n",
       " 'attempted': 351,\n",
       " 'use': 4720,\n",
       " 'metaphor': 2843,\n",
       " 'uncomfortable': 4663,\n",
       " 'go': 1946,\n",
       " 'end': 1483,\n",
       " 'realize': 3584,\n",
       " 'stupid': 4292,\n",
       " 'never': 3028,\n",
       " 'get': 1920,\n",
       " 'minutes': 2880,\n",
       " 'back': 386,\n",
       " 'surely': 4351,\n",
       " 'house': 2206,\n",
       " 'alone': 210,\n",
       " 'belongs': 462,\n",
       " 'pre': 3392,\n",
       " 'cable': 646,\n",
       " 'tv': 4634,\n",
       " 'days': 1158,\n",
       " 'eager': 1413,\n",
       " 'offer': 3114,\n",
       " 'alternative': 216,\n",
       " 'popular': 3359,\n",
       " 'shows': 4006,\n",
       " 'made': 2714,\n",
       " 'thriller': 4501,\n",
       " 'cast': 714,\n",
       " 'credible': 1073,\n",
       " 'situations': 4054,\n",
       " 'plays': 3328,\n",
       " 'high': 2128,\n",
       " 'school': 3865,\n",
       " 'student': 4283,\n",
       " 'gets': 1921,\n",
       " 'series': 3937,\n",
       " 'threatening': 4497,\n",
       " 'letters': 2598,\n",
       " 'everyone': 1560,\n",
       " 'seems': 3911,\n",
       " 'think': 4482,\n",
       " 'nothing': 3074,\n",
       " 'really': 3587,\n",
       " 'scared': 3856,\n",
       " 'tony': 4541,\n",
       " 'bill': 483,\n",
       " 'play': 3322,\n",
       " 'travolta': 4590,\n",
       " 'john': 2423,\n",
       " 'sister': 4045,\n",
       " 'principal': 3433,\n",
       " 'dennis': 1207,\n",
       " 'quaid': 3522,\n",
       " 'roles': 3762,\n",
       " 'rich': 3725,\n",
       " 'kid': 2479,\n",
       " 'competent': 928,\n",
       " 'still': 4235,\n",
       " 'relevant': 3641,\n",
       " 'social': 4094,\n",
       " 'lovely': 2688,\n",
       " '30': 55,\n",
       " 'older': 3128,\n",
       " 'among': 232,\n",
       " 'usually': 4728,\n",
       " 'gives': 1936,\n",
       " 'moving': 2950,\n",
       " 'performance': 3251,\n",
       " 'nice': 3035,\n",
       " 'little': 2639,\n",
       " 'steven': 4229,\n",
       " 'fighting': 1723,\n",
       " 'martial': 2766,\n",
       " 'stuff': 4288,\n",
       " 'movies': 2949,\n",
       " 'without': 4922,\n",
       " 'great': 1985,\n",
       " 'watched': 4842,\n",
       " 'couple': 1039,\n",
       " 'weeks': 4866,\n",
       " 'ago': 173,\n",
       " 'must': 2975,\n",
       " 'say': 3850,\n",
       " 'side': 4012,\n",
       " 'comes': 901,\n",
       " 'performances': 3252,\n",
       " 'good': 1957,\n",
       " 'br': 561,\n",
       " 'discussion': 1309,\n",
       " 'board': 522,\n",
       " 'found': 1831,\n",
       " 'review': 3714,\n",
       " 'poster': 3380,\n",
       " 'captured': 678,\n",
       " 'points': 3345,\n",
       " 'says': 3852,\n",
       " 'raises': 3549,\n",
       " 'questions': 3528,\n",
       " 'hardly': 2065,\n",
       " 'answers': 265,\n",
       " 'disturbing': 1324,\n",
       " 'every': 1557,\n",
       " 'experiment': 1604,\n",
       " 'conventional': 997,\n",
       " 'drama': 1360,\n",
       " 'despite': 1235,\n",
       " 'scenes': 3863,\n",
       " 'acting': 115,\n",
       " 'barbara': 403,\n",
       " 'partly': 3213,\n",
       " 'impressive': 2274,\n",
       " 'editing': 1438,\n",
       " 'camera': 659,\n",
       " 'gross': 2000,\n",
       " 'holes': 2162,\n",
       " 'technical': 4426,\n",
       " 'especially': 1538,\n",
       " 'sound': 4125,\n",
       " 'however': 2210,\n",
       " 'boredom': 544,\n",
       " 'audiences': 361,\n",
       " 'deal': 1162,\n",
       " '40': 60,\n",
       " 'remains': 3650,\n",
       " 'unfortunately': 4686,\n",
       " 'true': 4619,\n",
       " 'intrigued': 2351,\n",
       " 'protagonists': 3478,\n",
       " 'cold': 878,\n",
       " 'less': 2590,\n",
       " 'drugs': 1390,\n",
       " 'times': 4522,\n",
       " 'direction': 1284,\n",
       " 'seemed': 3909,\n",
       " 'virtually': 4782,\n",
       " 'non': 3056,\n",
       " 'existent': 1592,\n",
       " 'mention': 2830,\n",
       " 'aspect': 331,\n",
       " 'quality': 3524,\n",
       " 'behind': 451,\n",
       " 'reviewer': 3715,\n",
       " 'said': 3822,\n",
       " 'somehow': 4107,\n",
       " 'clear': 840,\n",
       " 'hanging': 2053,\n",
       " 'mid': 2852,\n",
       " 'air': 182,\n",
       " 'thus': 4512,\n",
       " 'blame': 501,\n",
       " 'almost': 209,\n",
       " 'long': 2663,\n",
       " 'flaws': 1772,\n",
       " 'sean': 3888,\n",
       " 'clearly': 841,\n",
       " 'english': 1498,\n",
       " 'character': 763,\n",
       " 'mute': 2977,\n",
       " 'brother': 597,\n",
       " 'developed': 1246,\n",
       " 'moments': 2909,\n",
       " 'promising': 3468,\n",
       " 'though': 4490,\n",
       " 'boy': 558,\n",
       " 'intensity': 2335,\n",
       " 'reached': 3571,\n",
       " 'whole': 4888,\n",
       " 'regret': 3627,\n",
       " 'chances': 753,\n",
       " 'given': 1935,\n",
       " 'away': 377,\n",
       " 'agree': 174,\n",
       " 'final': 1738,\n",
       " 'faces': 1638,\n",
       " 'compared': 924,\n",
       " 'suffer': 4320,\n",
       " 'thirty': 4486,\n",
       " 'staged': 4186,\n",
       " 'perfectly': 3249,\n",
       " 'edited': 1437,\n",
       " 'cup': 1108,\n",
       " 'tea': 4416,\n",
       " 'controversial': 996,\n",
       " 'idea': 2235,\n",
       " 'interesting': 2342,\n",
       " 'practically': 3389,\n",
       " 'maybe': 2795,\n",
       " 'bigger': 481,\n",
       " 'budget': 609,\n",
       " 'experienced': 1602,\n",
       " 'become': 439,\n",
       " 'ocean': 3107,\n",
       " 'twelve': 4635,\n",
       " 'plain': 3311,\n",
       " 'bad': 390,\n",
       " 'two': 4644,\n",
       " 'robbery': 3749,\n",
       " '10': 4,\n",
       " 'known': 2506,\n",
       " 'weak': 4853,\n",
       " 'script': 3883,\n",
       " 'slow': 4078,\n",
       " 'developing': 1247,\n",
       " 'saw': 3849,\n",
       " '20': 40,\n",
       " 'wrong': 4975,\n",
       " 'may': 2794,\n",
       " 'films': 1737,\n",
       " 'heist': 2104,\n",
       " 'theme': 4473,\n",
       " '12': 7,\n",
       " 'making': 2734,\n",
       " 'far': 1672,\n",
       " 'superior': 4342,\n",
       " 'kudos': 2512,\n",
       " 'apart': 277,\n",
       " 'hit': 2148,\n",
       " 'lies': 2606,\n",
       " 'happened': 2056,\n",
       " 'much': 2956,\n",
       " 'career': 687,\n",
       " 'shame': 3966,\n",
       " 'fault': 1686,\n",
       " 'excellent': 1573,\n",
       " 'starring': 4199,\n",
       " 'elizabeth': 1465,\n",
       " 'montgomery': 2916,\n",
       " 'release': 3638,\n",
       " 'dvd': 1407,\n",
       " 'form': 1817,\n",
       " 'earlier': 1415,\n",
       " 'also': 215,\n",
       " 'rape': 3558,\n",
       " 'soul': 4123,\n",
       " 'available': 371,\n",
       " 'fans': 1668,\n",
       " 'believe': 456,\n",
       " 'actress': 122,\n",
       " 'role': 3761,\n",
       " 'result': 3699,\n",
       " 'perhaps': 3258,\n",
       " 'trilogy': 4609,\n",
       " 'legend': 2579,\n",
       " 'released': 3639,\n",
       " 'record': 3606,\n",
       " 'straight': 4255,\n",
       " 'highly': 2133,\n",
       " 'regard': 3622,\n",
       " 'abilities': 82,\n",
       " 'seen': 3912,\n",
       " 'utter': 4729,\n",
       " 'complete': 934,\n",
       " 'live': 2641,\n",
       " 'france': 1837,\n",
       " 'turned': 4630,\n",
       " 'front': 1860,\n",
       " 'line': 2625,\n",
       " 'awful': 380,\n",
       " 'cool': 1007,\n",
       " 'forest': 1810,\n",
       " 'different': 1268,\n",
       " 'start': 4201,\n",
       " 'girl': 1931,\n",
       " 'meets': 2813,\n",
       " 'guy': 2028,\n",
       " 'messages': 2840,\n",
       " 'shut': 4008,\n",
       " 'bathroom': 420,\n",
       " 'closet': 860,\n",
       " 'ludicrous': 2702,\n",
       " 'starts': 4204,\n",
       " 'downhill': 1350,\n",
       " 'quickly': 3530,\n",
       " 'desperate': 1233,\n",
       " 'process': 3447,\n",
       " 'ancient': 238,\n",
       " 'ugly': 4652,\n",
       " 'creatures': 1071,\n",
       " 'coming': 907,\n",
       " 'past': 3225,\n",
       " 'kill': 2483,\n",
       " 'head': 2087,\n",
       " 'heck': 2103,\n",
       " 'bring': 585,\n",
       " 'monster': 2913,\n",
       " 'look': 2665,\n",
       " 'dressed': 1372,\n",
       " 'bother': 550,\n",
       " 'climax': 850,\n",
       " 'goofy': 1959,\n",
       " 'laughed': 2549,\n",
       " 'battle': 422,\n",
       " 'young': 4990,\n",
       " 'women': 4928,\n",
       " 'appear': 286,\n",
       " 'expert': 1606,\n",
       " 'kung': 2513,\n",
       " 'fu': 1866,\n",
       " 'masters': 2780,\n",
       " 'professor': 3457,\n",
       " 'surprised': 4354,\n",
       " 'supposed': 4348,\n",
       " 'god': 1948,\n",
       " 'war': 4824,\n",
       " 'anyway': 274,\n",
       " 'death': 1169,\n",
       " 'throws': 4510,\n",
       " 'pitiful': 3304,\n",
       " 'sight': 4016,\n",
       " 'brief': 579,\n",
       " 'mayhem': 2796,\n",
       " 'mysterious': 2979,\n",
       " 'murders': 2966,\n",
       " 'la': 2517,\n",
       " 'clumsy': 869,\n",
       " 'eaten': 1428,\n",
       " 'fake': 1655,\n",
       " 'wannabe': 4819,\n",
       " 'sh': 3959,\n",
       " 'learned': 2567,\n",
       " 'lousy': 2684,\n",
       " 'study': 4287,\n",
       " 'university': 4698,\n",
       " 'survive': 4363,\n",
       " 'mass': 2773,\n",
       " 'disney': 1315,\n",
       " 'guilty': 2021,\n",
       " 'cash': 712,\n",
       " 'cow': 1048,\n",
       " 'disease': 1310,\n",
       " 'success': 4311,\n",
       " 'bug': 612,\n",
       " '1968': 22,\n",
       " 'mouse': 2941,\n",
       " 'rides': 3730,\n",
       " 'goes': 1950,\n",
       " 'neither': 3020,\n",
       " 'sequel': 3932,\n",
       " 'capturing': 680,\n",
       " 'charm': 772,\n",
       " 'appeal': 284,\n",
       " 'find': 1742,\n",
       " 'race': 3539,\n",
       " 'driver': 1382,\n",
       " 'jim': 2416,\n",
       " 'douglas': 1348,\n",
       " 'sidekick': 4013,\n",
       " 'naturally': 2999,\n",
       " 'outside': 3163,\n",
       " 'mixed': 2898,\n",
       " 'diamond': 1259,\n",
       " 'falls': 1659,\n",
       " 'car': 681,\n",
       " 'stunts': 4291,\n",
       " 'course': 1042,\n",
       " 'pleasant': 3329,\n",
       " 'easy': 1426,\n",
       " 'eye': 1631,\n",
       " 'friendly': 1856,\n",
       " 'nowhere': 3082,\n",
       " 'fast': 1680,\n",
       " 'personally': 3265,\n",
       " 'bath': 419,\n",
       " 'dance': 1134,\n",
       " 'lady': 2524,\n",
       " '4': 59,\n",
       " 'late': 2541,\n",
       " 'night': 3040,\n",
       " 'put': 3517,\n",
       " 'expecting': 1599,\n",
       " 'laugh': 2546,\n",
       " 'martin': 2767,\n",
       " 'lawrence': 2556,\n",
       " 'comic': 904,\n",
       " 'actor': 120,\n",
       " 'might': 2855,\n",
       " 'stand': 4188,\n",
       " 'comedian': 896,\n",
       " 'style': 4294,\n",
       " 'richard': 3726,\n",
       " 'concert': 950,\n",
       " 'full': 1868,\n",
       " 'racist': 3543,\n",
       " 'directed': 1282,\n",
       " 'white': 4886,\n",
       " 'india': 2293,\n",
       " 'indians': 2295,\n",
       " 'clichéd': 846,\n",
       " 'philosophy': 3277,\n",
       " 'black': 495,\n",
       " 'comics': 906,\n",
       " 'directors': 1288,\n",
       " 'resist': 3688,\n",
       " 'urge': 4717,\n",
       " 'need': 3012,\n",
       " 'king': 2494,\n",
       " 'civil': 828,\n",
       " 'rights': 3735,\n",
       " 'struggle': 4278,\n",
       " 'comedy': 900,\n",
       " 'show': 3999,\n",
       " 'designed': 1229,\n",
       " 'skin': 4061,\n",
       " 'listening': 2635,\n",
       " 'funny': 1874,\n",
       " 'boring': 545,\n",
       " 'halfway': 2038,\n",
       " 'either': 1456,\n",
       " 'birth': 488,\n",
       " 'child': 794,\n",
       " 'humour': 2220,\n",
       " 'indulgent': 2299,\n",
       " 'part': 3209,\n",
       " 'bore': 542,\n",
       " 'details': 1242,\n",
       " 'cut': 1117,\n",
       " 'section': 3901,\n",
       " 'rubbish': 3790,\n",
       " 'surprise': 4353,\n",
       " 'mad': 2713,\n",
       " 'saturday': 3843,\n",
       " 'spike': 4157,\n",
       " 'host': 2200,\n",
       " 'tell': 4440,\n",
       " 'getting': 1922,\n",
       " 'used': 4721,\n",
       " 'imagine': 2260,\n",
       " 'difficult': 1270,\n",
       " 'writer': 4970,\n",
       " 'unlike': 4701,\n",
       " 'talk': 4400,\n",
       " 'let': 2595,\n",
       " 'ride': 3729,\n",
       " 'figure': 1725,\n",
       " 'next': 3034,\n",
       " 'quick': 3529,\n",
       " 'minded': 2868,\n",
       " 'segment': 3914,\n",
       " 'hurt': 2228,\n",
       " 'kinda': 2492,\n",
       " 'sexy': 3957,\n",
       " 'type': 4646,\n",
       " 'hard': 2062,\n",
       " 'eyes': 1633,\n",
       " 'exact': 1568,\n",
       " 'episode': 1522,\n",
       " 'date': 1147,\n",
       " 'favorite': 1688,\n",
       " 'loved': 2687,\n",
       " 'idiot': 2241,\n",
       " 'j': 2385,\n",
       " 'holmes': 2167,\n",
       " 'living': 2644,\n",
       " 'intelligent': 2332,\n",
       " 'liberal': 2603,\n",
       " 'sense': 3925,\n",
       " 'impression': 2273,\n",
       " 'painful': 3189,\n",
       " 'got': 1964,\n",
       " 'paid': 3187,\n",
       " 'food': 1798,\n",
       " 'sets': 3946,\n",
       " 'built': 618,\n",
       " 'current': 1112,\n",
       " 'state': 4205,\n",
       " 'world': 4951,\n",
       " 'associated': 338,\n",
       " 'include': 2281,\n",
       " 'viewing': 4769,\n",
       " 'public': 3495,\n",
       " 'deserve': 1225,\n",
       " 'whatever': 4879,\n",
       " 'happens': 2058,\n",
       " 'bored': 543,\n",
       " 'conversations': 999,\n",
       " 'minimal': 2873,\n",
       " 'aware': 376,\n",
       " 'fact': 1641,\n",
       " 'lost': 2678,\n",
       " 'come': 895,\n",
       " 'short': 3992,\n",
       " 'sit': 4047,\n",
       " 'die': 1263,\n",
       " 'across': 112,\n",
       " 'accident': 100,\n",
       " 'incredible': 2289,\n",
       " 'worthy': 4961,\n",
       " 'praise': 3391,\n",
       " 'critics': 1090,\n",
       " 'site': 4050,\n",
       " 'terrific': 4454,\n",
       " 'innocent': 2312,\n",
       " 'boris': 546,\n",
       " 'sweet': 4376,\n",
       " 'help': 2109,\n",
       " 'feel': 1700,\n",
       " 'father': 1685,\n",
       " 'serve': 3940,\n",
       " 'others': 3157,\n",
       " 'moved': 2944,\n",
       " 'reveals': 3711,\n",
       " 'family': 1664,\n",
       " 'broken': 594,\n",
       " 'trust': 4621,\n",
       " 'marry': 2762,\n",
       " 'twisted': 4642,\n",
       " 'mouth': 2942,\n",
       " 'betrayal': 472,\n",
       " 'fear': 1692,\n",
       " 'rest': 3697,\n",
       " 'visiting': 4786,\n",
       " 'destruction': 1239,\n",
       " 'worse': 4956,\n",
       " 'destroyed': 1238,\n",
       " 'descent': 1218,\n",
       " 'smoking': 4087,\n",
       " 'since': 4035,\n",
       " 'french': 1850,\n",
       " 'cinema': 818,\n",
       " 'latter': 2545,\n",
       " 'happen': 2055,\n",
       " 'thankfully': 4466,\n",
       " 'tears': 4425,\n",
       " 'heart': 2095,\n",
       " 'musician': 2973,\n",
       " 'soldier': 4098,\n",
       " 'saved': 3846,\n",
       " 'returns': 3707,\n",
       " 'breaks': 571,\n",
       " 'news': 3032,\n",
       " 'describe': 1219,\n",
       " 'finds': 1744,\n",
       " 'grab': 1969,\n",
       " 'yet': 4988,\n",
       " 'home': 2171,\n",
       " 'buried': 625,\n",
       " 'want': 4820,\n",
       " 'touches': 4555,\n",
       " 'soviet': 4133,\n",
       " 'political': 3349,\n",
       " 'revealed': 3709,\n",
       " 'piano': 3288,\n",
       " 'playing': 3327,\n",
       " 'anti': 267,\n",
       " 'someone': 4108,\n",
       " 'steals': 4219,\n",
       " 'wife': 4894,\n",
       " 'doubt': 1346,\n",
       " 'conclusion': 951,\n",
       " 'ending': 1486,\n",
       " 'finally': 1740,\n",
       " 'learns': 2569,\n",
       " 'certain': 740,\n",
       " 'dead': 1160,\n",
       " 'manages': 2741,\n",
       " 'future': 1876,\n",
       " 'survivors': 4365,\n",
       " 'james': 2394,\n",
       " 'top': 4544,\n",
       " 'till': 4518,\n",
       " 'second': 3894,\n",
       " 'third': 4485,\n",
       " 'jimmy': 2417,\n",
       " 'arrives': 314,\n",
       " 'hands': 2050,\n",
       " 'jumps': 2447,\n",
       " 'feet': 1704,\n",
       " 'national': 2996,\n",
       " 'dialogue': 1257,\n",
       " 'expected': 1598,\n",
       " 'crisp': 1086,\n",
       " 'efforts': 1450,\n",
       " 'place': 3307,\n",
       " 'conscious': 964,\n",
       " 'studio': 4285,\n",
       " 'run': 3798,\n",
       " 'requires': 3683,\n",
       " 'notion': 3077,\n",
       " 'juvenile': 2455,\n",
       " 'turn': 4629,\n",
       " 'spare': 4139,\n",
       " 'frankie': 1843,\n",
       " 'kinds': 2493,\n",
       " 'trouble': 4616,\n",
       " 'rotten': 3780,\n",
       " 'street': 4263,\n",
       " 'ideal': 2236,\n",
       " 'handsome': 2051,\n",
       " 'charismatic': 768,\n",
       " 'nose': 3065,\n",
       " 'could': 1031,\n",
       " 'steel': 4220,\n",
       " 'today': 4529,\n",
       " 'touch': 4553,\n",
       " 'oscar': 3155,\n",
       " 'best': 470,\n",
       " 'foreign': 1809,\n",
       " 'stop': 4246,\n",
       " 'nights': 3044,\n",
       " 'event': 1553,\n",
       " 'key': 2473,\n",
       " 'originality': 3153,\n",
       " 'country': 1036,\n",
       " 'warm': 4827,\n",
       " 'thought': 4491,\n",
       " 'provoking': 3488,\n",
       " 'mentally': 2829,\n",
       " 'ill': 2249,\n",
       " 'man': 2737,\n",
       " 'insane': 2314,\n",
       " 'insight': 2317,\n",
       " 'question': 3527,\n",
       " 'whether': 4884,\n",
       " 'lives': 2643,\n",
       " 'huge': 2212,\n",
       " 'remake': 3651,\n",
       " 'sadly': 3818,\n",
       " 'box': 557,\n",
       " 'office': 3118,\n",
       " 'failure': 1649,\n",
       " 'track': 4564,\n",
       " 'riveting': 3746,\n",
       " 'entertainment': 1515,\n",
       " 'cure': 1109,\n",
       " 'nevertheless': 3029,\n",
       " 'provided': 3485,\n",
       " 'escape': 1535,\n",
       " 'afternoon': 163,\n",
       " 'albeit': 190,\n",
       " 'fantasy': 1671,\n",
       " 'expect': 1596,\n",
       " 'corny': 1018,\n",
       " 'thrills': 4504,\n",
       " 'occasional': 3101,\n",
       " 'police': 3346,\n",
       " 'inspector': 2318,\n",
       " 'name': 2985,\n",
       " 'called': 652,\n",
       " 'solve': 4105,\n",
       " 'murder': 2962,\n",
       " 'entertaining': 1514,\n",
       " 'delivers': 1201,\n",
       " 'romance': 3768,\n",
       " 'marriage': 2760,\n",
       " 'seem': 3908,\n",
       " 'towards': 4561,\n",
       " 'truth': 4622,\n",
       " 'technique': 4428,\n",
       " 'experience': 1601,\n",
       " 'patience': 3229,\n",
       " 'master': 2777,\n",
       " 'bag': 392,\n",
       " 'dirty': 1291,\n",
       " 'tricks': 4606,\n",
       " 'background': 388,\n",
       " 'music': 2970,\n",
       " 'grand': 1974,\n",
       " 'noir': 3051,\n",
       " 'wonderful': 4931,\n",
       " 'visual': 4787,\n",
       " 'eventually': 1555,\n",
       " 'mystery': 2980,\n",
       " 'hey': 2122,\n",
       " 'alas': 189,\n",
       " 'keep': 2464,\n",
       " 'secret': 3898,\n",
       " 'beyond': 476,\n",
       " 'closing': 861,\n",
       " 'credits': 1076,\n",
       " 'u': 4650,\n",
       " 'k': 2456,\n",
       " 'featuring': 1698,\n",
       " 'often': 3122,\n",
       " '1': 3,\n",
       " 'star': 4195,\n",
       " 'sad': 3816,\n",
       " 'fat': 1682,\n",
       " 'ages': 171,\n",
       " 'adults': 145,\n",
       " 'recommend': 3603,\n",
       " 'years': 4982,\n",
       " 'laughs': 2551,\n",
       " 'pretty': 3418,\n",
       " 'marty': 2768,\n",
       " 'hilarious': 2134,\n",
       " 'overall': 3166,\n",
       " 'nicely': 3036,\n",
       " 'enjoy': 1499,\n",
       " 'concept': 945,\n",
       " 'terrible': 4452,\n",
       " 'advance': 146,\n",
       " 'laughable': 2547,\n",
       " 'reaches': 3572,\n",
       " 'elsewhere': 1467,\n",
       " 'worth': 4958,\n",
       " 'hour': 2204,\n",
       " 'talking': 4402,\n",
       " 'grasp': 1981,\n",
       " 'anne': 259,\n",
       " 'superbly': 4340,\n",
       " 'femme': 1709,\n",
       " 'sign': 4017,\n",
       " 'crap': 1054,\n",
       " 'unbelievable': 4660,\n",
       " 'although': 217,\n",
       " 'prefer': 3399,\n",
       " 'classical': 836,\n",
       " 'write': 4969,\n",
       " 'comment': 910,\n",
       " 'sentence': 3929,\n",
       " 'core': 1014,\n",
       " 'enough': 1506,\n",
       " 'unnecessary': 4704,\n",
       " 'words': 4943,\n",
       " '11': 6,\n",
       " 'rented': 3667,\n",
       " 'yesterday': 4987,\n",
       " 'touching': 4556,\n",
       " 'face': 1636,\n",
       " 'inspiration': 2319,\n",
       " 'care': 685,\n",
       " 'amount': 234,\n",
       " 'language': 2535,\n",
       " 'understand': 4671,\n",
       " 'magnificent': 2721,\n",
       " 'rough': 3781,\n",
       " 'tough': 4557,\n",
       " 'mean': 2798,\n",
       " 'disturbed': 1323,\n",
       " 'witty': 4925,\n",
       " 'somewhat': 4112,\n",
       " 'quirky': 3533,\n",
       " 'realizes': 3586,\n",
       " 'soon': 4118,\n",
       " 'boys': 560,\n",
       " 'abandoned': 80,\n",
       " 'church': 816,\n",
       " 'saying': 3851,\n",
       " 'amusing': 236,\n",
       " 'seeing': 3904,\n",
       " 'grown': 2007,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-gram\n",
    "vectorizer2 = TfidfVectorizer(stop_words=stop_words, max_features=5000, ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "X_train2 = vectorizer.fit_transform(x_train).toarray()\n",
    "X_test2 = vectorizer.fit_transform(x_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train2.shape)\n",
    "print(X_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題3】TF-IDFを用いた学習\n",
    "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。モデルは2値分類が行える任意のものを利用してください。\n",
    "\n",
    "\n",
    "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.51632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"accuracy\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9194\n"
     ]
    }
   ],
   "source": [
    "#2-gram\n",
    "clf2 = SGDClassifier()\n",
    "clf.fit(X_train2, y_train)\n",
    "y_pred2 = clf.predict(X_test2)\n",
    "print(\"accuracy\", accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題4】TF-IDFのスクラッチ実装\n",
    "以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。\n",
    "\n",
    "This movie is SOOOO funny!!!  \n",
    "What a movie! I never  \n",
    "best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This movie is SOOOO funny!!!',\n",
       " 'What a movie! I never',\n",
       " 'best movie ever!!!!! this movie']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>soooo</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this  movie  is  soooo  funny  what  a  i  never  best  ever\n",
       "0     1      1   1      1      1     0  0  0      0     0     0\n",
       "1     0      1   0      0      0     1  1  1      1     0     0\n",
       "2     1      2   0      0      0     0  0  0      0     1     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 0.4054651081081644, 'movie': 0.0, 'is': 1.0986122886681098, 'soooo': 1.0986122886681098, 'funny': 1.0986122886681098, 'what': 1.0986122886681098, 'a': 1.0986122886681098, 'i': 1.0986122886681098, 'never': 1.0986122886681098, 'best': 1.0986122886681098, 'ever': 1.0986122886681098}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>soooo</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       this  movie        is     soooo     funny      what         a  \\\n",
       "0  0.081093    0.0  0.219722  0.219722  0.219722  0.000000  0.000000   \n",
       "1  0.000000    0.0  0.000000  0.000000  0.000000  0.219722  0.219722   \n",
       "2  0.081093    0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          i     never      best      ever  \n",
       "0  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.219722  0.219722  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.219722  0.219722  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#標準的な式\n",
    "\n",
    "df = pd.DataFrame(index=range(len(corpus)))\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    text = text.lower() #小文字に\n",
    "    text = text.replace(\"!\", \"\") #！消す\n",
    "    words = text.split(\" \") #分割 & リスト化\n",
    "    for word in words:\n",
    "        if word not in df.columns:\n",
    "            df[word] = 0\n",
    "        df.loc[i, word] += 1\n",
    "\n",
    "display(df)\n",
    "\n",
    "\n",
    "N = df.shape[0] #サンプル数\n",
    "idf_dic = {}\n",
    "for word in df.columns:\n",
    "    idf_dic[word] = np.log(N/(df[word]>0).sum())\n",
    "print(idf_dic)\n",
    "    \n",
    "for j in range(N):\n",
    "    token_num = np.sum(df.loc[j, :])\n",
    "    for word in df.columns:\n",
    "        df.loc[j, word] = (df.loc[j, word]/token_num) * idf_dic[word]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全文書に登場する単語のTF-IDFが強制的に0になってしまう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>soooo</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   this  movie  is  soooo  funny  what  a  i  never  best  ever\n",
       "0     1      1   1      1      1     0  0  0      0     0     0\n",
       "1     0      1   0      0      0     1  1  1      1     0     0\n",
       "2     1      2   0      0      0     0  0  0      0     1     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1.2876820724517808, 'movie': 1.0, 'is': 1.6931471805599454, 'soooo': 1.6931471805599454, 'funny': 1.6931471805599454, 'what': 1.6931471805599454, 'a': 1.6931471805599454, 'i': 1.6931471805599454, 'never': 1.6931471805599454, 'best': 1.6931471805599454, 'ever': 1.6931471805599454}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>this</th>\n",
       "      <th>movie</th>\n",
       "      <th>is</th>\n",
       "      <th>soooo</th>\n",
       "      <th>funny</th>\n",
       "      <th>what</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>never</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.287682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.287682</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       this  movie        is     soooo     funny      what         a  \\\n",
       "0  1.287682    1.0  1.693147  1.693147  1.693147  0.000000  0.000000   \n",
       "1  0.000000    1.0  0.000000  0.000000  0.000000  1.693147  1.693147   \n",
       "2  1.287682    2.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          i     never      best      ever  \n",
       "0  0.000000  0.000000  0.000000  0.000000  \n",
       "1  1.693147  1.693147  0.000000  0.000000  \n",
       "2  0.000000  0.000000  1.693147  1.693147  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#scikit-learnの式\n",
    "\n",
    "df = pd.DataFrame(index=range(len(corpus)))\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    text = text.lower() #小文字に\n",
    "    text = text.replace(\"!\", \"\") #！消す\n",
    "    words = text.split(\" \") #分割 & リスト化\n",
    "    for word in words:\n",
    "        if word not in df.columns:\n",
    "            df[word] = 0\n",
    "        df.loc[i, word] += 1\n",
    "\n",
    "display(df)\n",
    "\n",
    "N = df.shape[0] #サンプル数\n",
    "idf_dic = {}\n",
    "for word in df.columns:\n",
    "    idf_dic[word] = np.log((1 + N)/(1 + (df[word]>0).sum())) + 1\n",
    "print(idf_dic)\n",
    "\n",
    "for j in range(N):\n",
    "    token_num = np.sum(df.loc[j, :])\n",
    "    for word in df.columns:\n",
    "        df.loc[j, word] = df.loc[j, word] * idf_dic[word]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全文書に登場する単語のTF-IDFが強制的に0になってしまうのを回避できている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "ニューラルネットワークを用いてベクトル化を行う手法が Word2Vec です。\n",
    "\n",
    "\n",
    "BoWやTF-IDFはone-hot表現であったため、得られるベクトルの次元は語彙数分になります。そのため、語彙数を増やしにくいという問題があります。一方で、Word2Vecでは単語を任意の次元のベクトルに変換します。これをを Word Embedding（単語埋め込み） や 分散表現 と呼びます。変換操作を「ベクトル空間に埋め込む」と言うことが多いです。\n",
    "\n",
    "\n",
    "Word2VecにはCBoWとSkip-gramという2種類の仕組みがあるため順番に見ていきます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBoW\n",
    "CBoW (Continuous Bag-of-Words) によるWord2Vecではある単語とある単語の間に来る単語を推定できるように全結合層2層のニューラルネットワークを学習します。\n",
    "\n",
    "\n",
    "単語はコーパスの語彙数次元のone-hot表現を行なっておきます。そのため、入力と出力の次元は語彙数と同じになります。一方で、中間のノード数をWord2Vecにより得たい任意の次元数とします。これにより全結合層の重みは「得たい次元のノード数×語彙数」になります。このネットワークにより学習を行なった後、出力側の重みを取り出すことで、各語彙を表すベクトルを手に入れることができます。\n",
    "\n",
    "\n",
    "間の単語の推定を行なっているため、同じ箇所で代替可能な言葉は似たベクトルになるというメリットもあります。これはBoWやTF-IDFでは得られない情報です。\n",
    "\n",
    "\n",
    "あるテキストは「そのテキストの長さ（単語数）×Word2Vecで得た分散表現の次元数」の配列になりますが、各入力の配列を揃える必要があるモデルに入力するためには、短いテキストは空白を表す単語を加える パディング を行なったり、長いテキストは単語を消したりします。テキストを 固定長 にすると呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ウィンドウサイズ\n",
    "入力する単語は推定する前後1つずつだけでなく、複数個とする場合もあります。前後いくつを見るかの大きさを ウィンドウサイズ と呼びます。\n",
    "\n",
    "\n",
    "##### Skip-gram\n",
    "CBoWとは逆にある単語の前後の単語を推定できるように全結合層2層のニューラルネットワークを学習する方法が Skip-gram です。学習を行なった後は入力側の重みを取り出し各語彙を表すベクトルとします。現在一般的に使われているのはCBoWよりもSki-gramです。\n",
    "\n",
    "\n",
    "##### 利用方法\n",
    "Pythonでは Gensim ライブラリを用いて扱うことができます。\n",
    "\n",
    "\n",
    "[gensim: models.word2vec – Word2vec embeddings](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "\n",
    "BoWの例と同じ文章で学習してみます。CountVectorizerと異なり前処理を自動的に行なってはくれないため、単語（トークン）はリストで分割しておきます。また、大文字は小文字に揃え、記号は取り除きます。\n",
    "\n",
    "\n",
    "デフォルトのパラメータではCBoWで計算されます。また、ウィンドウサイズはwindow=5に設定されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'very', 'good', 'film', 'a', 'bad'])\n",
      "thisのベクトル : \n",
      "[ 0.02365869  0.01886323  0.04146154  0.04979543  0.01740373  0.0331458\n",
      " -0.02010566 -0.030622    0.00138941 -0.03772479]\n",
      "movieのベクトル : \n",
      "[-0.00515686  0.04369088  0.0420732  -0.02897665 -0.01872913 -0.00971327\n",
      "  0.00768237 -0.03534362 -0.03133305 -0.00465968]\n",
      "isのベクトル : \n",
      "[ 0.00457812 -0.0066611   0.02874527 -0.0370437   0.02399253  0.04535765\n",
      " -0.03786702  0.01229602  0.03732948  0.04804269]\n",
      "veryのベクトル : \n",
      "[ 0.04414469 -0.00664242 -0.00087459  0.01801981 -0.00988795 -0.04676831\n",
      "  0.00826601  0.02250074  0.01416051 -0.02492178]\n",
      "goodのベクトル : \n",
      "[ 0.01435657  0.0199935  -0.03111928  0.04428221 -0.00873459 -0.04095155\n",
      " -0.01677543  0.01484518 -0.02207142  0.04147479]\n",
      "filmのベクトル : \n",
      "[ 0.0323323  -0.01237822  0.03803379  0.04172546 -0.04646122 -0.0227507\n",
      " -0.03762242 -0.02792635  0.01614347  0.04127774]\n",
      "aのベクトル : \n",
      "[-0.00560524 -0.02327583 -0.01647522  0.00833912 -0.02729021 -0.03370345\n",
      " -0.03917094 -0.03654372  0.01931227 -0.04882482]\n",
      "badのベクトル : \n",
      "[ 0.03386854  0.03964428  0.03733436 -0.043827    0.04094407 -0.02494766\n",
      " -0.03705006  0.02805403 -0.01830634  0.04094841]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "for vocab in model.wv.vocab.keys():\n",
    "    print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 単語の距離\n",
    "ベクトル間で計算を行うことで、ある単語に似たベクトルを持つ単語を見つけることができます。例えばgoodに似たベクトルの単語を3つ探します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.40011265873908997),\n",
       " ('very', 0.3142697811126709),\n",
       " ('bad', 0.19806697964668274)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"good\", topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今の例では3文しか学習していませんので効果を発揮しませんが、大きなコーパスで学習することで、並列関係のものが近くに来たりなど面白い結果が得られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 可視化\n",
    "2次元に圧縮することで単語ごとの位置関係を可視化することができます。以下はt-SNEを用いた例です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "vocabs = model.wv.vocab.keys()\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB映画レビューデータセットの分散表現\n",
    "\n",
    "IMDB映画レビューデータセットの訓練データをコーパスとしてWord2Vecを学習させ分散表現を獲得しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題5】コーパスの前処理\n",
    "コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "preprocess_x = x_train\n",
    "for i in range(len(x_train)):\n",
    "    temp = preprocess_x[i].lower()\n",
    "    temp = temp.replace(\"<br />\", \"\").strip()\n",
    "    temp = re.sub(r\"(https?|ftp)(:\\/\\/[-_\\.!~*\\'()a-zA-Z0-9;\\/?:\\@&=\\+$,%#]+)\", \"\" ,temp)\n",
    "    temp = re.sub(r'[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％]', \"\" ,temp)\n",
    "    temp = temp.split()\n",
    "    preprocess_x[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zero',\n",
       " 'day',\n",
       " 'leads',\n",
       " 'you',\n",
       " 'to',\n",
       " 'think,',\n",
       " 'even',\n",
       " 're-think',\n",
       " 'why',\n",
       " 'two',\n",
       " 'boys/young',\n",
       " 'men',\n",
       " 'would',\n",
       " 'do',\n",
       " 'what',\n",
       " 'they',\n",
       " 'did',\n",
       " '-',\n",
       " 'commit',\n",
       " 'mutual',\n",
       " 'suicide',\n",
       " 'via',\n",
       " 'slaughtering',\n",
       " 'their',\n",
       " 'classmates.',\n",
       " 'it',\n",
       " 'captures',\n",
       " 'what',\n",
       " 'must',\n",
       " 'be',\n",
       " 'beyond',\n",
       " 'a',\n",
       " 'bizarre',\n",
       " 'mode',\n",
       " 'of',\n",
       " 'being',\n",
       " 'for',\n",
       " 'two',\n",
       " 'humans',\n",
       " 'who',\n",
       " 'have',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'withdraw',\n",
       " 'from',\n",
       " 'common',\n",
       " 'civility',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'define',\n",
       " 'their',\n",
       " 'own/mutual',\n",
       " 'world',\n",
       " 'via',\n",
       " 'coupled',\n",
       " 'destruction.it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'perfect',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'given',\n",
       " 'what',\n",
       " 'money/time',\n",
       " 'the',\n",
       " 'filmmaker',\n",
       " 'and',\n",
       " 'actors',\n",
       " 'had',\n",
       " '-',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'remarkable',\n",
       " 'product.',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'explaining',\n",
       " 'the',\n",
       " 'motives',\n",
       " 'and',\n",
       " 'actions',\n",
       " 'of',\n",
       " 'the',\n",
       " 'two',\n",
       " 'young',\n",
       " 'suicide/murderers',\n",
       " 'it',\n",
       " 'is',\n",
       " 'better',\n",
       " 'than',\n",
       " \"'elephant'\",\n",
       " '-',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'being',\n",
       " 'a',\n",
       " 'film',\n",
       " 'that',\n",
       " 'gets',\n",
       " 'under',\n",
       " 'our',\n",
       " \"'rationalistic'\",\n",
       " 'skin',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'far,',\n",
       " 'far',\n",
       " 'better',\n",
       " 'film',\n",
       " 'than',\n",
       " 'almost',\n",
       " 'anything',\n",
       " 'you',\n",
       " 'are',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'see.',\n",
       " 'flawed',\n",
       " 'but',\n",
       " 'honest',\n",
       " 'with',\n",
       " 'a',\n",
       " 'terrible',\n",
       " 'honesty.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_x_test = x_test\n",
    "for i in range(len(x_test)):\n",
    "    temp = preprocess_x_test[i].lower()\n",
    "    temp = temp.replace(\"<br />\", \"\").strip()\n",
    "    temp = re.sub(r\"(https?|ftp)(:\\/\\/[-_\\.!~*\\'()a-zA-Z0-9;\\/?:\\@&=\\+$,%#]+)\", \"\" ,temp)\n",
    "    temp = re.sub(r'[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％]', \"\" ,temp)\n",
    "    temp = temp.split()\n",
    "    preprocess_x_test[i] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題6】Word2Vecの学習\n",
    "Word2Vecの学習を行なってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22250705, 28713375)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = preprocess_x\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題7】（アドバンス課題）ベクトルの可視化\n",
    "得られたベクトルをt-SNEにより可視化してください。また、いくつかの単語を選びwv.most_similarを用いて似ている単語を調べてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('funny', 0.9667276740074158),\n",
       " ('bad,', 0.9565809965133667),\n",
       " ('scary', 0.9554940462112427)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"bad\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rock-stars', 0.9612486958503723),\n",
       " ('her.\"', 0.9555264115333557),\n",
       " ('share.jim', 0.9513508677482605)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"friends\", topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスの前処理が不十分そうではある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itonaoki/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-52a30dc17cfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvocabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtsne_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pca\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvectors_tsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_tsne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors_tsne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \"\"\"\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             distances_nn, neighbors_nn = knn.kneighbors(\n\u001b[0;32m--> 725\u001b[0;31m                 None, n_neighbors=k)\n\u001b[0m\u001b[1;32m    726\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    384\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             )\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocabs = model.wv.vocab.keys()\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=250, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間が長すぎるため、手動でストップ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
