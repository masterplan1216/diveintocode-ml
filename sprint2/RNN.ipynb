{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sprintの目的\n",
    "スクラッチを通してリカレントニューラルネットワークの基礎を理解する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#np.set_printoptions(threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リカレントニューラルネットワークスクラッチ\n",
    "\n",
    "リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n",
    "\n",
    "\n",
    "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "\n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "$$\n",
    "a_t = x_t ・ W_x + h_{t-1}・W_h + B\n",
    "$$\n",
    "$$\n",
    "h_t = tanh(a_t)\n",
    "$$\n",
    "\n",
    "$a_t$ : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "$h_t$ : 時刻tの状態・出力 (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "$x_{t}$ : 時刻tの入力 (batch_size, n_features)\n",
    "\n",
    "\n",
    "$W_{x}$ : 入力に対する重み (n_features, n_nodes)\n",
    "\n",
    "\n",
    "$h_{t-1}$ : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "$W_{h}$ : 状態に対する重み。 (n_nodes, n_nodes)\n",
    "\n",
    "\n",
    "$B$ : バイアス項 (n_nodes,)\n",
    "\n",
    "\n",
    "初期状態 $h_{0}$ は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
    "\n",
    "\n",
    "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 $x$ は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
    "\n",
    "\n",
    "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializer ############################\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray(n_nodes1, n_nodes2)\n",
    "        \"\"\"\n",
    "        W = self.sigma*np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray(1, n_nodes2)\n",
    "        \"\"\"\n",
    "        B = self.sigma*np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B\n",
    "\n",
    "# 活性化関数 ##########################\n",
    "class Tanh:\n",
    "    def forward(self, X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def _softmax(self, X):\n",
    "        X = X - np.max(X, axis=-1, keepdims=True)\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=-1, keepdims=True)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def forward(self, X, t):\n",
    "        self.t = t\n",
    "        self.y = self._softmax(X)\n",
    "        self.loss = self._cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def _cross_entropy_error(self, y, t):     \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dX = (self.y - self.t)/batch_size\n",
    "        return dX\n",
    "\n",
    "# Optimizer #########################\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - self.lr*layer.dW\n",
    "        layer.B = layer.B - self.lr*layer.dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, n_nodes, n_features, initializer=None, optimizer=SGD(), activation=Tanh()):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_features = n_features\n",
    "        if initializer is None:\n",
    "            self.Wx = w_x  # shape(n_features, n_nodes)\n",
    "            self.Wh = w_h  # shape(n_nodes, n_nodes)\n",
    "            self.B = b.astype(np.float)\n",
    "        else:\n",
    "            self.Wx = initializer.W(n_features, n_nodes)\n",
    "            self.Wh = initializer.W(n_nodes, n_nodes)\n",
    "            self.B = initializer.B(n_features, n_nodes)\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.batch_size = X.shape[0]\n",
    "        self.n_sequences = X.shape[1]\n",
    "        self.h = np.zeros((self.batch_size, self.n_nodes)) # (batch_size, n_nodes)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(self.n_sequences):\n",
    "                A = (X[i][j] @ self.Wx) + (self.h @ self.Wh) + self.B\n",
    "                self.h = self.activation.forward(A)\n",
    "        return self.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.01, 0.02],\n",
       "        [0.02, 0.03],\n",
       "        [0.03, 0.04]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.03, 0.05, 0.07],\n",
       "       [0.03, 0.05, 0.07, 0.08]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.03, 0.05, 0.07],\n",
       "       [0.02, 0.04, 0.06, 0.08],\n",
       "       [0.03, 0.05, 0.07, 0.08],\n",
       "       [0.04, 0.06, 0.08, 0.1 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**フォワードプロパゲーションの流れ確認**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0007, 0.0013, 0.0019, 0.0023])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x@wx\n",
    "x[0][0]@w_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0007, 1.0013, 1.0019, 1.0023])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x@wx + b\n",
    "x[0][0]@w_x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76188798, 0.76213958, 0.76239095, 0.76255841])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tanh(x@wx + b) = h_s1\n",
    "np.tanh(x[0][0]@w_x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07623574, 0.13721527, 0.19819481, 0.25155044])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#h_s1@w_h\n",
    "np.tanh(x[0][0]@w_x + b)@w_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07733574, 1.13931527, 1.20129481, 1.25535044])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x@wx + (h_s1@w_h) + b\n",
    "x[0][1]@w_x + np.tanh(x[0][0]@w_x + b)@w_h + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.792209  , 0.8141834 , 0.83404912, 0.84977719])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tanh(x@wx + (h_s1@w_h) + b) = h_s2\n",
    "np.tanh(x[0][1]@w_x + np.tanh(x[0][0]@w_x + b)@w_h + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08321832, 0.14902269, 0.21482707, 0.27229095])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#h_s2@w_h\n",
    "(np.tanh(x[0][1]@w_x + np.tanh(x[0][0]@w_x + b)@w_h + b))@w_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.08471832, 1.15192269, 1.21912707, 1.27759095])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x@wx + (h_s2@w_h) + b\n",
    "x[0][2]@w_x + (np.tanh(x[0][1]@w_x + np.tanh(x[0][0]@w_x + b)@w_h + b))@w_h + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79494228, 0.81839002, 0.83939649, 0.85584174])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tanh(x@wx + (h_s2@w_h) + b)\n",
    "np.tanh(x[0][2]@w_x + (np.tanh(x[0][1]@w_x + np.tanh(x[0][0]@w_x + b)@w_h + b))@w_h + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = SimpleRNN(n_nodes, n_features)\n",
    "rnn.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**diverの例と一致していることを確認**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
    "バックプロパゲーションを実装してください。\n",
    "\n",
    "\n",
    "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
